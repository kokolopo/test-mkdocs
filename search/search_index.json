{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<ul> <li>llm model (openrouter), select model, price, policy</li> <li>framework (langchain ecosystem), select framework</li> <li>vector db (Qdrant), select vector db</li> <li>backend (fastapi/python), select backend</li> <li> <p>protocol (MCP), select protocol</p> </li> <li> <p>HR APP for employee performance management, data restriction, access control, audit logging</p> </li> <li>Access Control: Implement role-based access control (RBAC) to ensure that only authorized users can access and perform actions within the application.</li> </ul>"},{"location":"admonitions/","title":"Admonitions","text":"<p>Example of an admonition/callout with a title:</p> <p>Title of the callout</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Collapsible callout:</p> Collapsible callout <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"code-examples/","title":"Code examples","text":"<p>An example of a codeblock for Python:</p> add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> <p>Example codeblock for JavaScript with lines highlighted:</p> concatenate_strings.js<pre><code>// Function to concatenate two strings\nfunction concatenateStrings(str1, str2) {\n  return str1 + str2;\n}\n\n// Example usage\nconst result = concatenateStrings(\"Hello, \", \"World!\");\nconsole.log(\"The concatenated string is:\", result);\n</code></pre>"},{"location":"content-tabs/","title":"Content tabs","text":""},{"location":"content-tabs/#content-tabs","title":"Content Tabs","text":"<p>This is some examples of content tabs.</p>"},{"location":"content-tabs/#generic-content","title":"Generic Content","text":"Plain textUnordered listOrdered list <p>This is some plain text</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item</li> </ul> <ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"content-tabs/#code-blocks-in-content-tabs","title":"Code Blocks in Content Tabs","text":"PythonJavaScript <pre><code>def main():\n    print(\"Hello world!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>function main() {\n    console.log(\"Hello world!\");\n}\n\nmain();\n</code></pre>"},{"location":"diagram-examples/","title":"Diagram Examples","text":""},{"location":"diagram-examples/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"diagram-examples/#sequence-diagrams","title":"Sequence Diagrams","text":"<pre><code>sequenceDiagram\n  autonumber\n  Server-&gt;&gt;Terminal: Send request\n  loop Health\n      Terminal-&gt;&gt;Terminal: Check for health\n  end\n  Note right of Terminal: System online\n  Terminal--&gt;&gt;Server: Everything is OK\n  Terminal-&gt;&gt;Database: Request customer data\n  Database--&gt;&gt;Terminal: Customer data</code></pre>"},{"location":"introductions/","title":"Introduction of Agentic AI","text":""},{"location":"introductions/#overview-of-agentic-ai","title":"Overview of Agentic AI","text":"<p>Secara umum, Agentic AI adalah sistem kecerdasan buatan yang lebih dari sekadar merespons permintaan (prompt). Ia memiliki kemampuan untuk:</p> <ul> <li>Berperilaku secara autonom (mandiri), artinya bisa melakukan tindakan tanpa arahan manusia di setiap langkah \u2014 manusia menyediakan tujuan / konteks, dan AI ini bekerja untuk mencapainya. (TechTarget)</li> <li>Mengambil keputusan sendiri berdasarkan pemahaman konteks dan data lingkungan (termasuk umpan balik). (AtScale)</li> <li>Merencanakan / membagi tugas menjadi subtugas, beradaptasi jika keadaan berubah, belajar dari interaksi / hasil sebelumnya (\u201clearning / feedback loops\u201d). (atfinity.swiss)</li> <li>Berinteraksi tidak hanya dengan permintaan manusia, tetapi juga dengan sistem, alat/aplikasi lain (API, sensor, database, dsb), sehingga bisa bertindak dalam lingkungan nyata atau sistem informasi. (Solo.io)</li> </ul>"},{"location":"introductions/#pengertian-menurut-beberapa-ahli-organisasi-artikel","title":"Pengertian Menurut Beberapa Ahli / Organisasi / Artikel","text":"<p>Berikut definisi atau penjelasan Agentic AI dari beberapa sumber:</p> Sumber Definisi / Penjelasan IBM \u2013 \u201cWhat is Agentic AI?\u201d \u201cAgentic AI adalah sistem AI yang bisa mencapai tujuan spesifik dengan supervisi terbatas. Terdiri dari agen-AI \u2014 model pembelajaran mesin yang meniru pengambilan keputusan manusia untuk menyelesaikan masalah secara real time.\u201d (IBM) Salesforce (Agentforce) AI yang menggunakan agen-memandu mandiri \u2014 model yang menyelesaikan tugas secara otomatis dan mengelola alur kerja, membuat keputusan waktu nyata berdasarkan algoritma dan analitik prediktif. (Salesforce) Atfinity AI yang dirancang untuk beroperasi secara autonom, mengejar tujuan, membuat keputusan independen, serta berinteraksi secara dinamis dengan lingkungan atau sistem lain untuk mencapai tujuan. (atfinity.swiss) TechTarget Sistem AI yang mampu mengambil tindakan dan keputusan secara mandiri, menangani situasi kompleks, dan menyesuaikan perilakunya secara otomatis berdasarkan data lingkungan. (TechTarget) McKinsey / Solo.io Menurut Solo.io, Agentic AI adalah sistem berbasis model generatif AI (foundation model) yang bisa bertindak di dunia nyata dan mengeksekusi proses multi-langkah. (Solo.io) Endava AI dengan otonomi yang lebih tinggi, kemampuan pengambilan keputusan &amp; adaptasi; AI yang diberikan persona, belajar dari data, memodifikasi perilaku seiring waktu; mengejar goal kompleks dan workflow dengan supervisi manusia terbatas. (endava.com)"},{"location":"introductions/#karakteristik-elemen-penting-dari-agentic-ai","title":"Karakteristik / Elemen Penting dari Agentic AI","text":"<p>Dari berbagai definisi tersebut, ada beberapa elemen yang sering muncul sebagai ciri Agentic AI:</p> <ol> <li>Autonomi \u2014 tidak terus-menerus diarahkan manusia, bisa mengambil inisiatif. (TechTarget)</li> <li>Penetapan Tujuan (Goal-driven) \u2014 sistem diberi tujuan tinggi dan bisa bekerja menuju tujuan tersebut. (Solo.io)</li> <li>Perencanaan &amp; Pemecahan Tugas \u2014 membagi tugas besar menjadi subtugas, merencanakan, dan menyusun strategi. (atfinity.swiss)</li> <li>Adaptabilitas / Belajar dari Umpan Balik \u2014 mampu menyesuaikan tindakan berdasarkan perubahan data, hasil, atau kondisi lingkungan. (TechTarget)</li> <li>Interaksi dengan Alat / Lingkungan \u2014 penggunaan API, alat eksternal, sensor, sistem lain, bukan hanya menghasilkan output teks/statistik. (Salesforce)</li> <li>Kesadaran Konteks (Context Awareness) \u2014 memahami lingkungan kerja atau domain, mempertimbangkan konteks dalam membuat keputusan/tindakan. (AtScale)</li> </ol>"},{"location":"introductions/#perbandingan-singkat-agentic-ai-vs-ai-tradisional-generatif-ai","title":"Perbandingan Singkat: Agentic AI vs AI Tradisional / Generatif AI","text":"<p> Gambar diatas menampilkan perbandingan tiga jenis AI: Agentic AI, Generative AI, dan Traditional AI, dilihat dari tiga aspek utama yaitu Primary Function (Fungsi Utama), Autonomy (Tingkat Otonomi), dan Learning (Pembelajaran).</p>"},{"location":"introductions/#primary-function-fungsi-utama","title":"Primary Function (Fungsi Utama)","text":"<ul> <li>Agentic AI \u2192 Fokus pada aksi dan pengambilan keputusan yang berorientasi tujuan. AI ini tidak hanya merespons, tetapi aktif mencapai suatu target.</li> <li>Generative AI \u2192 Fokus pada pembuatan konten (teks, kode, gambar, video, dsb.) berdasarkan data latih.</li> <li>Traditional AI \u2192 Fokus pada otomatisasi tugas repetitif sesuai algoritma/rule yang sudah ditentukan.</li> </ul>"},{"location":"introductions/#autonomy-otonomi","title":"Autonomy (Otonomi)","text":"<ul> <li>Agentic AI \u2192 Tinggi, dapat beroperasi dengan pengawasan manusia yang minimal. Bisa mengambil keputusan sendiri.</li> <li>Generative AI \u2192 Bervariasi, karena sering membutuhkan prompt atau arahan dari manusia.</li> <li>Traditional AI \u2192 Rendah, sangat bergantung pada algoritma dan aturan yang telah diprogram.</li> </ul>"},{"location":"introductions/#learning-pembelajaran","title":"Learning (Pembelajaran)","text":"<ul> <li>Agentic AI \u2192 Menggunakan Reinforced Learning, artinya belajar dari pengalaman dan memperbaiki kinerja seiring waktu.</li> <li>Generative AI \u2192 Menggunakan Data-driven learning, belajar dari data historis/eksisting untuk menghasilkan keluaran baru.</li> <li>Traditional AI \u2192 Tidak belajar secara dinamis, hanya mengandalkan aturan yang sudah ditentukan sebelumnya dan membutuhkan intervensi manusia untuk perubahan.</li> </ul>"},{"location":"introductions/ai-agent/","title":"What is AI Agent?","text":""},{"location":"introductions/ai-agent/#overview-of-ai-agent","title":"Overview of AI Agent","text":"<p>AI Agent merupakan entitas AI (program/sistem) yang melakukan tugas tertentu secara otomatis, biasanya dalam domain yang cukup spesifik dan dengan aturan atau alur kerja yang telah ditetapkan. Ia bisa merespons input, melakukan aksi berdasarkan skenario tertentu, dan seringkali memerlukan trigger atau arahan dari manusia. (AI21)</p>"},{"location":"introductions/ai-agent/#components-of-ai-agent","title":"Components of AI Agent","text":"<p>Gambar diatas merupakan komponen-komponen utama pembangun sebuah AI Agent:</p> <ul> <li>Input \u2014 data atau permintaan dari pengguna / lingkungan</li> <li>LLMs (Large Language Models) \u2014 inti pemahaman dan generation (berpikir / reasoning)</li> <li>Tools \u2014 alat / fungsi yang bisa dipanggil untuk menyelesaikan aksi spesifik</li> <li>Memory \u2014 penyimpanan informasi dari masa lalu untuk data, konteks, personalisasi, dan kesinambungan</li> <li>Guardrails \u2014 lapisan pengaman (aturan, batasan) agar agent tidak melakukan sesuatu yang tidak diinginkan</li> </ul> <p>Berikut fungsi dan pentingnya masing-masing komponen:</p> Komponen Fungsi / Peran LLMs Sebagai \u201cotak\u201d agent: memproses input \u2192 memahami konteks \u2192 merumuskan respons / keputusan. Ini termasuk reasoning, perencanaan, inferensi. (livechatai.com) Input / Persepsi Agent perlu mendapat informasi dari luar (pengguna, API, sensor, data environment) supaya bisa merespons atau bertindak berdasarkan situasi terkini. (rivista.ai) Tools Memungkinkan agent melakukan aksi konkret (contoh: mengambil data lewat API, menjalankan fungsi eksternal, mengakses database). Tools ini memperluas kemampuan agent di luar generasi teks sederhana. (livechatai.com) Memory Penting agar agent tidak \u201chilang konteks.\u201d Ada short-term memory (untuk percakapan atau aksi berjalan), long-term memory (untuk menyimpan preferensi, pengalaman masa lalu), kadang ada episodic memory. Memory mendukung kontinuitas, personalisasi, dan belajar dari pengalaman. (Intelligence Strategy) Guardrails Pengamanan agar agent tetap aman, etis, dan sesuai tujuan. Bisa berupa aturan otomatis, validasi input/output, pembatasan penggunaan tool, mekanisme koreksi bila agent keluar dari batasan yang diizinkan. (GoCodeo)"},{"location":"introductions/ai-agent/#bagaimana-komponen-komponen-berinteraksi","title":"Bagaimana Komponen-Komponen Berinteraksi","text":"<p>Secara garis besar, alur sistem kerja agent kira-kira sebagai berikut:</p> <ol> <li>Persepsi / Input masuk \u2192 agent melihat/mendengar/terima permintaan dari user atau environment.</li> <li>LLMs / reasoning / decision logic memproses input, membaca memory untuk konteks lama, menimbang opsi, merencanakan langkah apa yang harus diambil.</li> <li>Jika diperlukan, agent memanggil tools untuk melakukan aksi nyata (akses data, operasi API, dll).</li> <li>Memory diperbarui: menyimpan hasil, keputusan, input/output supaya di masa depan agent bisa lebih baik atau mempertahankan konteks.</li> <li>Di sepanjang proses, guardrails aktif menjaga bahwa agent tidak melakukan tindakan yang melanggar kebijakan, aman, valid, tidak bias, etc.</li> </ol>"},{"location":"introductions/ai-agent/#kenapa-komponen-komponen-tersebut-penting","title":"Kenapa Komponen-Komponen Tersebut Penting","text":"<ul> <li>Tanpa memory, agent jadi \u201cstateless\u201d \u2192 tidak punya kapasitas untuk mempertahankan konteks antar interaksi, pengalaman, atau preferensi pengguna. (Intelligence Strategy)</li> <li>Tanpa tools / fungsi eksternal, kemampuan aksi agent terbatas hanya pada generasi teks/ide, tidak bisa melakukan tindakan nyata di dunia luar. (livechatai.com)</li> <li>Tanpa guardrails, risiko agent melakukan kesalahan, keluar dari batasan etika / keamanan / privasi sangat tinggi. (arXiv)</li> </ul>"},{"location":"introductions/ai-agent/#korelasi-dengan-agentic-ai","title":"Korelasi Dengan Agentic AI","text":"<p>Dari berbagai sumber, berikut bagaimana kedua konsep ini saling berkaitan:</p> Aspek Bagaimana AI Agent terkait dengan Agentic AI Sub-komponen Sebuah AI Agent bisa menjadi bagian dari sistem Agentic AI. Artinya, Agentic AI sering menggunakan banyak AI agents untuk menangani berbagai sub-tugas dalam workflow yang lebih kompleks. (scrapeless.com) Skala &amp; Kompleksitas AI agent biasanya lebih terbatas pada tugas tunggal atau domain yang spesifik; Agentic AI mengangkat level kompleksitasnya, mengoordinasikan banyak agent, memecah tugas ke sub-tugas, dan mengelola dependensi serta interaksi antar agen. (scrapeless.com) Otonomi &amp; Adaptasi AI agent bisa punya otonomi dalam lingkup tugasnya (misalnya menentukan bagaimana menyelesaikan tugas berdasarkan input), tetapi seringnya tidak punya otonomi tinggi / tidak selalu adaptif ke kondisi di luar skenario yang dilatih. Agentic AI menekankan adaptasi, pembelajaran terus-menerus, merespons kondisi baru, dan bahkan menghasilkan strategi sendiri untuk mencapai tujuan akhir. (Freshworks) Tujuan &amp; Perencanaan AI Agent lebih cenderung mempunyai tujuan kecil atau tugas langsung (contoh: menjawab email, melakukan scheduling, menjawab FAQ). Agentic AI mengandung kemampuan untuk menyusun tujuan jangka panjang, merencanakan aksi multi-langkah, dan menyusun taktik / strategi untuk mencapai tujuan tersebut. (AI21) Koordinasi &amp; Orkestrasi Agentic AI biasanya memiliki lapisan orkestrasi (orchestration layer) yang mengatur interaksi antar agen-agen, memonitor progres, memberikan supervisi bila perlu, mengatur kolaborasi agen agen agar mencapai hasil yang harmonis. AI agents sendiri mungkin bekerja independen jika tidak dalam sistem Agentic AI. (scrapeless.com)"},{"location":"introductions/llms/llm/","title":"Large Language Models (LLMs)","text":""},{"location":"introductions/llms/llm/#overview-of-llms","title":"Overview of LLMs","text":"<p>Large Language Model adalah model AI berukuran besar yang dilatih pada sejumlah besar teks dengan metode pembelajaran self-supervised atau pra-latih, tanpa label eksplisit, sehingga bisa mempelajari struktur bahasa, semantik, sintaksis, dan pengetahuan tersirat dalam data teks. (Wikipedia) Fokusnya biasanya pada tugas pemrosesan bahasa alami (NLP): penerjemahan, ringkasan, menjawab pertanyaan, membuat teks kreatif, bahkan kode. (Swimm)</p>"},{"location":"introductions/llms/llm/#cara-kerja-dasar","title":"Cara Kerja Dasar","text":"<p>Gambar menunjukan langkah-langkah utama LLM saat menerima teks input dan menghasilkan teks output. Berikut tiap tahapannya:</p> <ol> <li> <p>Input Text, Pengguna memasukkan teks, misalnya pertanyaan atau perintah.</p> </li> <li> <p>Tokenizer, Teks dipecah menjadi bagian\u2010bagian kecil yang disebut token (bisa kata, subkata, atau karakter tergantung model). Setiap token dikonversi ke bentuk angka (ID token).</p> </li> <li> <p>Embedding, ID token tersebut diubah menjadi vektor numerik (embedding) \u2014 vektor yang mewakili makna token dalam ruang dimensi tinggi. Juga ditambah informasi posisi (positional encoding) supaya model \u201ctahu\u201d urutan token dalam kalimat.</p> </li> <li> <p>Encoder / Decoder / Mechanism (tergantung arsitektur)</p> <ol> <li> <p>Dalam banyak LLM generatif (seperti GPT), model menggunakan bagian decoder-only yang memproses embedding dan menghasilkan token satu-persatu.</p> </li> <li> <p>Jika arsitekturnya \u201cencoder-decoder\u201d (seperti di model penerjemahan), input diproses oleh encoder untuk membuat representasi kontekstual, kemudian decoder menggunakannya untuk menghasilkan output.</p> </li> </ol> </li> <li> <p>Attention Mechanism, Ini adalah bagian kunci, memungkinkan model memperhatikan bagian lain dari input untuk memahami konteks.</p> <ol> <li> <p>Self-attention \u2192 setiap token melihat token-token lain di sekitarnya (termasuk sebelumnya dan sesudahnya dalam input) untuk memahami konteks secara keseluruhan.</p> </li> <li> <p>Masked attention (pada decoder) \u2192 ketika menghasilkan output token satu-per-satu, model tidak boleh melihat token yang akan dihasilkan di masa depan (agar tidak \u201ccurang\u201d).</p> </li> </ol> </li> <li> <p>Output Text, Setelah pemrosesan melalui beberapa lapisan (transformer blocks), model memprediksi token selanjutnya berdasarkan probabilitas. Token hasil prediksi kemudian diubah kembali menjadi teks. Proses ini berulang sampai selesai (bisa sampai token \u201cend\u201d atau berdasarkan panjang maksimal).</p> </li> </ol>"},{"location":"introductions/llms/llm/#contoh-sederhana-ilustratif","title":"Contoh Sederhana Ilustratif","text":"<p>Bayangkan model mendapati input:</p> <p>\u201cCuaca di Padang hari ini seperti apa?\u201d</p> <p>Alurnya kira-kira:</p> <ul> <li>Input \u2192 dipecah jadi token: \u201cCuaca\u201d, \u201cdi\u201d, \u201cPadang\u201d, \u201chari\u201d, \u201cini\u201d, \u201cseperti\u201d, \u201capa\u201d, \u201c?\u201d</li> <li>Token tersebut \u2192 diubah menjadi angka \u2192 embedding + posisi</li> <li>Attention melihat kata \u201cCuaca\u201d serta \u201cPadang\u201d dan \u201chari ini\u201d untuk memahami konteks: model tahu \u201cPadang hari ini\u201d adalah lokasi dan waktu, sehingga konteks cuaca lokal sekarang.</li> <li>Decoder / proses prediksi \u2192 memproduksi kata satu demi satu, mungkin: \u201cHari ini cuaca di Padang cerah dengan sedikit awan.\u201d</li> </ul>"},{"location":"introductions/llms/llm/#keunggulan-manfaat","title":"Keunggulan / Manfaat","text":"<p>Beberapa manfaat besar dari LLM antara lain:</p> <ol> <li>Interaksi yang lebih natural dengan bahasa manusia \u2014 bisa mengerti konteks, idiom, nuansa, dan merespons seolah-olah \u201cmengerti\u201d. </li> <li>Fleksibilitas tugas: dari teks bebas sampai teknis seperti kode, ringkasan, penerjemahan, menjawab tanya, dll.</li> <li>Hemat dalam pengembangan: karena pra-latihan yang umum, bisa diadaptasi ke domain tertentu tanpa harus mulai dari nol tiap kali.</li> <li>Kapabilitas multibahasa dan gaya (style) yang bisa disesuaikan.</li> </ol>"},{"location":"introductions/llms/llm/#tantangan-dan-keterbatasan","title":"Tantangan dan Keterbatasan","text":"<p>Namun, LLM bukan tanpa masalah. Berikut beberapa hal yang perlu diperhatikan:</p> <ul> <li>Hallucinations: output yang terdengar meyakinkan tetapi salah secara fakta atau tidak berdasar.</li> <li>Bias &amp; etika: jika data latih memiliki bias (gender, budaya, ras, dsb.), model bisa memperkuat atau mereproduksi bias tersebut.</li> <li>Data kedaluwarsa / informasi yang statis: model pra-latih hanya pada waktu data dikumpulkan; kalau tidak diperbarui, bisa memberikan jawaban yang sudah usang untuk hal-hal terkini.</li> <li>Kebutuhan sumber daya besar: meliputi komputasi (GPU/TPU), penyimpanan besar, energi, biaya yang tinggi.</li> <li>Kesulitan interpretabilitas: sering disebut sebagai \u201cblack box\u201d \u2014 sulit menjelaskan kenapa model memilih output tertentu.</li> </ul>"},{"location":"introductions/llms/llm/#tren-masa-depan","title":"Tren &amp; Masa Depan","text":"<p>Beberapa arah pengembangan yang sedang aktif:</p> <ul> <li>Integrasi dengan alat eksternal dan basis data (retrieval-augmented generation / RAG) agar jawaban bisa lebih akurat dan berdasarkan fakta terkini.</li> <li>Model multimodal (bukan hanya teks, tapi juga gambar/suara/video) agar memahami dan menghasilkan konten yang lebih beragam.</li> <li>Teknik efisiensi: seperti model compression, pruning, quantization, agar model yang besar bisa lebih ringan dipakai di berbagai perangkat.</li> <li>Peningkatan aspek keamanan, fairness, auditabilitas, dan transparansi.</li> </ul>"},{"location":"introductions/llms/provider/","title":"Penyedia Large Language Models (LLMs) &amp; Platform Agregator","text":""},{"location":"introductions/llms/provider/#contoh-penyedia-model-open-source","title":"Contoh Penyedia / Model Open-Source","text":"<ul> <li>Mistral AI \u2014 punya model dengan parameter besar, open-source, berkompetisi dalam benchmark utama. (Wikipedia)</li> <li>DBRX (Databricks / Mosaic ML) \u2014 model \u201cmixture-of-experts\u201d yang memiliki 132 miliar parameter, punya varian yang instruction-tuned. (Wikipedia)</li> <li>Vicuna LLM \u2014 open-source model chat berbasis Llama 2, cukup populer di komunitas, dengan varian yang ditingkatkan (context window, kualitas respons). (Wikipedia)</li> <li>Gemma (DeepMind / Google) \u2014 seri model ringan/open-source dari DeepMind, alternatif model hyperskala untuk tugas tertentu. (Wikipedia)</li> </ul>"},{"location":"introductions/llms/provider/#contoh-platform-agregator-hosting-infrastruktur","title":"Contoh Platform Agregator / Hosting / Infrastruktur","text":"<ul> <li>AWS Bedrock \u2014 menyediakan akses ke berbagai foundation models (termasuk model pihak ketiga &amp; open-source) lewat satu API, plus fitur keamanan / compliance enterprise. (Eden AI)</li> <li>Hugging Face Inference Endpoints \u2014 memungkinkan pengembang menggunakan model-model open-source seperti Falcon, Mistral, Llama, dsb., dengan endpoint yang sudah dihosting. (Eden AI)</li> <li>LLM.co \u2014 menyediakan paket harga / deployment model yang bisa private / hybrid / on-premise. (llm.co)</li> <li>Lumio AI \u2014 platform multi-model; users bisa mengakses berbagai LLM dari satu antarmuka, membandingkan hasil, dan memilih model per tugas. (Wikipedia)</li> </ul>"},{"location":"introductions/llms/provider/#tabel-perbandingan-penyedia-platform-propertinya","title":"Tabel Perbandingan Penyedia / Platform &amp; Propertinya","text":"<p>Berikut tabel perbandingan sejumlah penyedia/provider &amp; platform agregator / open-source:</p> <p>Tentu, berikut adalah gabungan dari kedua tabel tersebut ke dalam satu format yang kohesif dan komprehensif.</p>"},{"location":"introductions/llms/provider/#perbandingan-penyedia-dan-platform-llm-utama","title":"Perbandingan Penyedia dan Platform LLM Utama","text":"Nama Platform Tipe Model / Layanan Unggulan (Ragam Model) Keunggulan Utama Catatan / Kekurangan OpenAI Komersial GPT-4 series, GPT-4 Turbo, GPT-3.5 Kualitas generasi bahasa tinggi, dukungan dokumentasi &amp; ekosistem besar. Biaya tinggi, penggunaan token banyak bisa mahal. Anthropic Komersial Claude (Sonnet, Opus, dll.) Fokus keamanan &amp; keandalan; model instruksi yang baik. Varian model terbaru mungkin punya latensi lebih tinggi; akses terbatas pada beberapa wilayah. Google / DeepMind Komersial &amp; Open-Source varian Gemini, Gemma Multibahasa &amp; multimodal; integrasi besar dengan ekosistem Google. Struktur harga dan kecepatan tergantung varian; tidak semua model gratis. Mistral AI Open-Source / Komersial (Hybrid) Model seperti Mistral-7B, Mixtral, varian reasoning. Sangat kompetitif di benchmark open-source; bisa self-host; fleksibel untuk developer. Infrastruktur &amp; pemeliharaan sendiri perlu biaya; dukungan enterprise bisa kurang. DBRX (Databricks) Open-Source / Komersial Model 132B, instruction-tuned. Performa tinggi; cocok untuk tugas kompleks / enterprise. Resource hardware &amp; biaya inferensi &amp; training tinggi. Lumio AI Platform Agregator / Antarmuka Multi-Model LLM dari berbagai penyedia (ChatGPT, Gemini, Claude, dll.) lewat satu interface. Mempermudah perbandingan; efisiensi; pengguna bisa model switching untuk biaya &amp; performa optimal. Ketergantungan pada API luar; mungkin latensi/variasi region; fitur keamanan/privasi tergantung model yang dipakai. OpenRouter Marketplace / Agregator Ratusan model dari penyedia seperti OpenAI, Anthropic, Meta, Mistral, dll. API tunggal untuk banyak model; pilihan varian free, extended, beta; kontrol rate &amp; kontrol model yang digunakan. Biaya tambahan (fee marketplace); ketersediaan model &amp; performa tergantung provider underlying; rate limit/free tier bisa rendah. AutoLLM Router Agregator / Router Model Tergantung konfigurasi (menyambungkan model berbeda berdasarkan kebutuhan). Pengoptimalan biaya &amp; performa; memilih model paling cocok untuk setiap permintaan. Harus konfigurasikan tiap use-case; mungkin overhead latensi / kompleksitas integrasi. AWS Bedrock Agregator / Hosting Enterprise Foundation models + model pihak ketiga / open-source besar. Keamanan &amp; kepatuhan; skalabilitas enterprise; integrasi mendalam dengan AWS. Biaya tinggi untuk penggunaan enterprise; butuh setup yang rumit; mungkin tidak fleksibel di beberapa lokasi/model. Hugging Face Inference Endpoints Hosting / Open-Source Mix Ribuan model open-source (Falcon, Llama, Mistral, dll.). Developer friendly; Komunitas &amp; dokumentasi besar; fleksibilitas besar dalam memilih model. Kinerja/Performa &amp; SLA tergantung tier/lokasi/region; biaya hosting/inferensi tetap ada."},{"location":"introductions/llms/provider/#tips-memilihagregasi","title":"Tips Memilih/Agregasi","text":"<p>Untuk memilih provider/platform/agregator yang paling cocok, pertimbangkan faktor-faktor berikut:</p> <ol> <li>Kebutuhan konteks &amp; ukuran token \u2014 seberapa besar konteks (jumlah token input + konteks historis) yang dibutuhkan tugasmu.</li> <li>Biaya &amp; anggaran \u2014 selain biaya per token, juga biaya infra (jika self-host), biaya latency / bandwidth, biaya pemeliharaan.</li> <li>Privasi &amp; kepatuhan regulasi \u2014 apakah data sensitif? Apakah lingkungan harus on-premises atau hybrid?</li> <li>Ketersediaan varian model &amp; fleksibilitas \u2014 apakah butuh model ringan vs besar; varian instruksi vs varian generatif vs varian reasoning; multibahasa.</li> <li>Support &amp; komunitas \u2014 dokumentasi, tools pendukung (debugging, prompt tuning, monitoring), keamanan &amp; guardrails, ekosistem integrasi.</li> </ol>"},{"location":"llm-self-hosting/","title":"Spesifikasi Teknologi","text":""},{"location":"llm-self-hosting/#rekomendasi-berdasarkan-skala-model-fokus-pada-vram-gpu","title":"Rekomendasi Berdasarkan Skala Model (Fokus pada VRAM GPU)","text":"Ukuran Model LLM (Parameter) Penggunaan VRAM Minimum (Inference/Running LLM) VRAM Minimum (Fine-Tuning/QLoRA) GPU yang Direkomendasikan ~7 Miliar (misalnya Llama 2 7B) Ringan/Eksperimen 8 GB - 12 GB 16 GB - 24 GB RTX 3060 (12GB), RTX 4070/4090 (24GB), Quadro RTX A4000 (16GB) ~13 Miliar (misalnya Llama 2 13B) Sedang 16 GB - 24 GB 24 GB - 40 GB RTX 4090 (24GB), A5000 (24GB), A6000 (48GB) ~34 Miliar (misalnya DeepSeek 34B) Berat/Profesional 40 GB - 50 GB 48 GB - 80 GB NVIDIA A6000 (48GB), A100 (40GB/80GB) ~70 Miliar (misalnya Llama 2 70B) Enterprise/Tingkat Lanjut 80 GB+ 80 GB+ (Multi-GPU) Multi-GPU A100 (80GB) atau H100 <p>Note</p> <ul> <li>Untuk Inference (menjalankan LLM), VRAM yang dibutuhkan adalah sekitar 1.2x hingga 1.6x ukuran model (dalam GB).</li> <li>Untuk Fine-Tuning (Full Fine-Tuning), VRAM yang dibutuhkan jauh lebih besar (hingga 16GB per 1 miliar parameter).</li> <li>Untuk Fine-Tuning yang efisien (QLoRA 4-bit), kebutuhan VRAM dapat sangat ditekan, menjadikannya pilihan realistis untuk GPU konsumen. Misalnya, model 7B hanya membutuhkan sekitar 5GB-9GB VRAM menggunakan QLoRA.</li> </ul>"},{"location":"llm-self-hosting/#spesifikasi-komponen-server-lain","title":"Spesifikasi Komponen Server Lain","text":"Komponen Rekomendasi Minimum (Skala Ringan/Eksperimen) Rekomendasi Ideal (Skala Sedang/Profesional) Fungsi RAM 32 GB DDR4/DDR5 64 GB atau 128 GB DDR5 Mendukung preprocessing data, data batching, dan mencegah out-of-memory pada CPU saat GPU kehabisan VRAM (offloading). CPU Intel Core i7/i9 (Generasi Terbaru) atau AMD Ryzen 7/9 (Multi-core) Intel Xeon atau AMD Threadripper (dengan core-count tinggi) Menangani data loading, preprocessing, dan sistem operasi. Harus memiliki core yang cukup untuk mendukung GPU. Penyimpanan (Storage) 1 TB NVMe SSD 2 TB+ NVMe SSD Kecepatan tinggi untuk loading dataset besar dan checkpoint model, NVMe SSD adalah wajib. Sistem Operasi Linux (Ubuntu/CentOS) Linux (Ubuntu) Paling optimal untuk deep learning dan dukungan driver GPU. <p>Tentu. Saya akan memberikan perkiraan harga untuk opsi GPU yang paling relevan untuk self-hosting LLM dan fine-tuning di Indonesia.</p> <p>Perkiraan ini didasarkan pada harga pasar retail baru dan bekas (jika tersedia), karena harga komponen server seringkali fluktuatif.</p>"},{"location":"llm-self-hosting/#perkiraan-harga-komponen-gpu-untuk-llm","title":"\ud83d\udcb0 Perkiraan Harga Komponen GPU untuk LLM","text":"<p>Asumsi pada GPU NVIDIA karena dukungan CUDA (Compute Unified Device Architecture) dan ketersediaan VRAM yang tinggi, yang krusial untuk LLM.</p>"},{"location":"llm-self-hosting/#opsi-1-performa-tinggi-terbaik-vram-24-gb","title":"Opsi 1: Performa Tinggi &amp; Terbaik (VRAM 24 GB)","text":"<p>Pilihan terbaik untuk fine-tuning model hingga 13B parameter (menggunakan QLoRA) dan inferensi model besar.</p> Komponen Spesifikasi Kunci Status Perkiraan Harga (IDR) Catatan NVIDIA GeForce RTX 4090 VRAM: 24 GB GDDR6X Baru Rp 27.000.000 - Rp 35.000.000+ GPU konsumen terbaik untuk LLM. Menawarkan keseimbangan harga/performa terbaik untuk VRAM 24 GB. NVIDIA GeForce RTX 3090 VRAM: 24 GB GDDR6X Bekas Rp 15.000.000 - Rp 20.000.000+ Pilihan bekas yang sangat baik dan masih powerful dengan VRAM 24 GB yang sama."},{"location":"llm-self-hosting/#opsi-2-keseimbangan-vram-16-gb-20-gb","title":"Opsi 2: Keseimbangan (VRAM 16 GB - 20 GB)","text":"<p>opsi yang masih mumpuni untuk inferensi model 7B-13B dan fine-tuning model 7B dengan QLoRA.</p> Komponen Spesifikasi Kunci Status Perkiraan Harga (IDR) Catatan NVIDIA GeForce RTX 4070 Ti SUPER VRAM: 16 GB GDDR6X Baru Rp 13.000.000 - Rp 16.000.000+ Pilihan modern yang lebih terjangkau dengan VRAM 16 GB. NVIDIA GeForce RTX 3060 VRAM: 12 GB GDDR6 Baru/Bekas Rp 4.000.000 - Rp 6.000.000+ Pilihan paling ekonomis yang dapat menjalankan LLM 7B terkuantisasi (4-bit/8-bit)."},{"location":"llm-self-hosting/#opsi-3-serverprofesional-vram-48-gb","title":"Opsi 3: Server/Profesional (VRAM 48 GB+)","text":"<p>Untuk menjalankan dan fine-tuning model di atas 34B parameter atau untuk kebutuhan performa enterprise.</p> Komponen Spesifikasi Kunci Status Perkiraan Harga (IDR) Catatan NVIDIA RTX A6000 / A5000 VRAM: 48 GB / 24 GB ECC Baru/Bekas Rp 45.000.000+ (A6000) Dirancang untuk beban kerja server dan memiliki VRAM sangat besar. Harga dapat bervariasi luas dan umumnya sulit dicari di pasar retail biasa. NVIDIA A100 / H100 VRAM: 40 GB - 80 GB HBM2/e Bekas/Khusus Rp 100.000.000++ GPU tingkat pusat data (data center). Biasanya hanya tersedia melalui distributor server khusus atau penyedia cloud."},{"location":"llm-self-hosting/#perkiraan-harga-server-lainnya","title":"\ud83d\udee0\ufe0f Perkiraan Harga Server Lainnya","text":"<p>Untuk kelengkapan server (menggunakan konfigurasi RTX 4090 sebagai basis):</p> Komponen Spesifikasi yang Direkomendasikan Perkiraan Harga (IDR) CPU AMD Ryzen 9 7900/Intel Core i7 Gen 14th Rp 6.000.000 - Rp 10.000.000 Motherboard (Sesuai CPU) dengan slot PCIe 4.0 x16 yang kuat Rp 3.000.000 - Rp 5.000.000 RAM 64 GB DDR5 (2x32GB) Rp 3.000.000 - Rp 5.000.000 Penyimpanan 2 TB NVMe Gen4 SSD Rp 1.500.000 - Rp 2.500.000 PSU (Power Supply) 1000W 80+ Platinum/Titanium (Wajib berkualitas tinggi) Rp 3.000.000 - Rp 5.000.000 Casing &amp; Cooling Server/PC Case dengan airflow baik + Liquid/Air Cooler Rp 2.000.000 - Rp 4.000.000"},{"location":"llm-self-hosting/installation-on-premise/","title":"Instalasi LLaMA 3 On-Premise di Linux","text":"<p>Dokumentasi ini menjelaskan langkah-langkah teknis untuk menginstal dan menjalankan model LLaMA\u00a03 secara lokal di Linux, serta mengekspose API publik-nya. Penjelasan mencakup instalasi dependensi, pengunduhan model, setup inferensi, hingga penyiapan server REST API dengan keamanan dasar.</p>"},{"location":"llm-self-hosting/installation-on-premise/#1-persiapan-sistem-dan-dependensi","title":"1. Persiapan Sistem dan Dependensi","text":"<ul> <li> <p>Python dan VirtualEnv: Pastikan Python terbaru (\u22653.9) terpasang, karena Transformers memerlukan Python 3.9 ke atas[1]. Buat lingkungan virtual untuk proyek agar dependensi terisolasi. Contoh perintah:     <pre><code>python3 -m venv .env  \nsource .env/bin/activate\n</code></pre>    (Perintah ini setara dengan contoh di dokumentasi Hugging Face[2].) Setelah aktif, pasang paket utama:     <pre><code>pip install transformers torch\n</code></pre>    Ini juga menginstal PyTorch (CPU) dan Hugging Face Transformers. Jika hanya CPU, bisa menggunakan indeks khusus PyTorch CPU; jika pakai GPU, pasang PyTorch GPU biasa.</p> </li> <li> <p>CUDA/cuDNN (jika GPU): Jika menggunakan GPU NVIDIA, pasang driver NVIDIA dan CUDA Toolkit yang sesuai untuk versi PyTorch [3]. Periksa deteksi GPU dengan <code>nvidia-smi</code>. Jika <code>nvidia-smi</code> tidak terdeteksi atau tidak ada GPU, gunakan PyTorch CPU-only. Setelah CUDA terinstal, <code>pip pip install torch</code> (versi CUDA) sudah mencakup cuDNN secara otomatis.</p> </li> <li> <p>Dependensi Tambahan: Jika menggunakan TGI (Text Generation Inference), instal Rust dan protokol Protobuf sesuai petunjuk TGI[4][5]. Contoh:     <pre><code>pip install text-generation-inference\n</code></pre></p> </li> <li>TGI membutuhkan Python 3.9+ dan Rust; dokumentasi resmi mencontohkan instalasi melalui <code>git clone</code> dan <code>make install</code>[4].</li> </ul>"},{"location":"llm-self-hosting/installation-on-premise/#2-unduh-dan-siapkan-model-llama-3","title":"2. Unduh dan Siapkan Model LLaMA\u00a03","text":"<ul> <li>Via Meta (download.sh): Kunjungi situs resmi LLaMA\u00a03 (llama.meta.com), setujui perjanjian lisensi, lalu jalankan skrip pengunduhan. Proses ini akan mengunduh berkas model dan tokenizer ke direktori lokal[6]. (Perhatikan link yang dikirim lewat email perlu dimasukkan saat diminta.)</li> <li>Via Hugging Face: Model LLaMA\u00a03 juga ada di Hugging Face (contoh: <code>meta-llama/Meta-Llama-3-8B-Instruct</code>). Setelah request akses disetujui, Anda dapat mengunduh model tersebut. Misalnya, menggunakan CLI Hugging Face:     <pre><code>huggingface-cli login  \nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct \\  \n--include \"original/*\" --local-dir Llama3-8B-Instruct\n</code></pre>     atau langsung menggunakan Transformers dalam kode Python:     <pre><code>from transformers import pipeline, torch\\_dtype  \nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  \npipeline = pipeline(  \n    \"text-generation\",  \n    model=model_id,  \n    model_kwargs={\"torch_dtype\": torch.bfloat16},  \n    device_map=\"auto\"  \n)\n</code></pre>     Contoh ini diadaptasi dari dokumentasi Meta Hugging Face[7]. Setelah diunduh (atau saat diakses pertama kali), model akan tersimpan di cache <code>~/.cache/huggingface</code> atau direktori yang ditentukan.</li> <li>Format Model: Pastikan model dalam format yang sesuai (misal file .pt, .bin, atau .safetensors dari Transformers; atau .gguf jika pakai llama.cpp). Jika perlu, konversi bisa dilakukan dengan skrip <code>convert_llama_ggml_to_gguf.py</code> dari repositori llama.cpp.</li> </ul>"},{"location":"llm-self-hosting/installation-on-premise/#3-framework-inferensi-llm","title":"3. Framework Inferensi LLM","text":"<p>Setelah model siap, gunakan framework inferensi untuk menjalankannya. Beberapa pilihan populer:</p> <ul> <li>Hugging Face Transformers (PyTorch): Pakai Transformers dengan pipeline atau model/tokenizer. Contoh:     <pre><code>from transformers import pipeline  \npipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\")  \noutput = pipe(\"Halo, bagaimana kabarmu?\")\n</code></pre>     Ini menggunakan Llama\u00a03 instruksi untuk teks generatif. Sama seperti contoh dokumentasi HF[7], pastikan <code>device_map=\"auto\"</code> agar model besar terdistribusi ke GPU jika tersedia.</li> <li>Text Generation Inference (TGI): TGI adalah toolkit Hugging Face untuk inference berperforma tinggi dengan REST API siap pakai[8]. Setelah instalasi (<code>pip install text-generation-inference</code>), jalankan server TGI dengan model Llama\u00a03 instruksi:     <pre><code>text-generation-launch --model-id meta-llama/Meta-Llama-3-8B-Instruct --tensor-parallel 1 --listen-port 8000\n</code></pre>     (Contoh ini mengikuti pola CLI TGI; dokumentasi resmi memberikan contoh serupa[9].) TGI mendukung optimasi produksi, batching, quantisasi, dan menyediakan metrik Prometheus bawaan[10].</li> <li>llama.cpp / llama-cpp-python: Untuk CPU (atau GPU terbatas), llama.cpp (C/C++ inference) mendukung LLaMA\u00a03[11]. Anda bisa menginstal binding Python-nya:     <pre><code>pip install llama-cpp-python[server]\n</code></pre>     Kemudian jalankan server API kompatibel OpenAI:     <pre><code>python3 -m llama_cpp.server --model path/to/model.gguf\n</code></pre>     Contoh ini diambil dari dokumentasi llama-cpp-python yang menunjukkan instalasi dan perintah server[12]. Setelah dijalankan, server akan expose endpoint mirip OpenAI (mis. /v1/completions).</li> <li>Alternatif Lain: Misal menggunakan Docker image atau layanan lain; utamakan framework yang sesuai kebutuhan. IntelliSense LLaMA-3 dapat dipanggil lewat pip llama-server atau tugas TorchRun dari repositori Llama\u00a03 Meta, tapi di sini fokus pada opsi di atas.</li> </ul>"},{"location":"llm-self-hosting/installation-on-premise/#4-menyiapkan-server-rest-api","title":"4. Menyiapkan Server REST API","text":"<p>Setelah model dan inferensi siap, buat service REST API untuk interaksi. Pendekatan umum:</p> <ul> <li>FastAPI + Transformers: Instal FastAPI dan server ASGI (uvicorn):     <pre><code>pip install fastapi uvicorn\n</code></pre>     Buat aplikasi (misal <code>app.py</code>):     <pre><code>from fastapi import FastAPI  \nfrom transformers import pipeline  \n\napp = FastAPI()  \ngenerator = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch\\_dtype=\"bfloat16\", device\\_map=\"auto\")  \n\n@app.post(\"/generate\")  \ndef generate(prompt: str):  \nresult = generator(prompt)  \nreturn {\"generated_text\": result[0][\"generated_text\"]}\n</code></pre></li> <li>Kode di atas menggunakan pipeline LLaMA\u00a03 instruksi sebagai model (diambil dari dokumentasi HF[7]). Jalankan server FastAPI dengan uvicorn:     <pre><code>uvicorn app:app --host 0.0.0.0 --port 8000\n</code></pre></li> <li>TGI REST Endpoint: Jika menggunakan TGI, Anda tidak perlu FastAPI terpisah. Cukup jalankan <code>text-generation-launch</code> seperti di atas, dan TGI secara otomatis menyediakan endpoint REST (JSON over HTTP) di port yang ditentukan. Klien dapat mengakses misal <code>http://localhost:8000/completions</code> (bergantung konfigurasi TGI). Dokumentasi TGI merekomendasikan Docker atau binary siap pakai untuk deployment produksi.</li> <li>llama-cpp-server: Perintah <code>python3 -m llama_cpp.server</code> seperti disebut di atas akan membuka HTTP server. Endpoint bawaannya kompatibel OpenAI API. Contoh saat dijalankan, Anda bisa <code>POST</code> ke <code>http://localhost:5000/v1/engines/llama_cpp/completions</code> (port default 5000) dengan payload JSON untuk mendapatkan hasil teks. Skrip <code>llama-cpp-python</code> menyediakan banyak opsi (lihat dokumentasi server-nya).</li> </ul> <p>Setelah server berjalan, lakukan tes dengan <code>curl</code> atau klien HTTP. Misal: <pre><code>    curl -X POST http://localhost:8000/generate -H \"Content-Type: application/json\" -d '{\"prompt\": \"Apa kabar?\"}'\n</code></pre></p> <p>Jika menggunakan FastAPI di atas, hasilnya akan berisi teks generatif.</p>"},{"location":"llm-self-hosting/installation-on-premise/#5-mengekspos-api-ke-publik","title":"5. Mengekspos API ke Publik","text":"<p>Agar API dapat diakses dari luar jaringan lokal, lakukan salah satu cara berikut:</p> <ul> <li>Reverse Proxy dengan NGINX: Pasang NGINX di server publik Anda. Buat konfigurasi virtual host yang mem-forward ke aplikasi. Contoh <code>/etc/nginx/sites-available/llama3</code>:     <pre><code>server {  \n    listen 80;  \n    server_name api.llama3.example.com;  \n\n    location / {  \n        proxy_pass http://127.0.0.1:8000;  \n        proxy_set_header Host $host;  \n        proxy_set_header X-Real-IP $remote_addr;  \n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  \n        proxy_set_header X-Forwarded-Proto $scheme;  \n    }  \n}\n</code></pre>     Contoh di atas mengarahkan <code>api.llama3.example.com</code> ke FastAPI/uvicorn di port 8000 pada localhost[13]. Aktifkan konfigurasi ini (<code>ln -s</code> ke <code>sites-enabled</code>), lalu reload NGINX:     <pre><code>sudo nginx -t  \nsudo systemctl reload nginx\n</code></pre>     Setelah itu, domain Anda akan melayani API. Untuk keamanan, gunakan HTTPS \u2013 misalnya dengan Certbot/Let\u2019s Encrypt:     <pre><code>sudo apt install certbot python3-certbot-nginx  \nsudo certbot --nginx -d api.llama3.example.com\n</code></pre>     Certbot akan mengonfigurasi SSL secara otomatis[14]. NGINX juga dapat diaplikasikan limit koneksi atau fitur WAF jika diperlukan.</li> <li>Tunneling (ngrok, Cloudflare Tunnel): Jika tidak ada server publik, gunakan tunnel. Contoh ngrok: jalankan     <pre><code>ngrok http 8000\n</code></pre>     yang akan memberikan URL publik (http/https) yang meneruskan ke localhost:8000[15]. Atau gunakan Cloudflare Tunnel (<code>cloudflared</code>): buat tunnel dan peta ke hostname pilihan, misal:     <pre><code>cloudflared tunnel --hostname llama3.example.com --url http://localhost:8000\n</code></pre>     Cloudflare akan meneruskan <code>llama3.example.com</code> ke server lokal tanpa membuka port secara langsung[16]. Keuntungan metode ini: IP dan port server Anda tidak terekspos langsung, dan Cloudflare otomatis menangani sertifikat HTTPS serta proteksi DDoS[16].</li> <li>DNS &amp; Firewall: Pastikan port yang digunakan (80/443) diizinkan lewat firewall/VPS. Jika menggunakan domain khusus, atur A-record ke IP server.</li> </ul>"},{"location":"llm-self-hosting/installation-on-premise/#6-keamanan-dasar","title":"6. Keamanan Dasar","text":"<ul> <li>Rate Limiting: Batasi laju request agar API tidak disalahgunakan atau overload. Jika menggunakan NGINX, aktifkan rate limit. Contoh konfigurasi NGINX:     <pre><code>http {  \n    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;  \n    ...  \n    server {  \n    ...  \n        location / {  \n            limit_req zone=mylimit burst=20 nodelay;  \n            proxy_pass http://127.0.0.1:8000;  \n        }  \n    }  \n}\n</code></pre>     Potongan di atas memastikan tiap IP maksimal ~10 request per detik[17], dengan kemampuan menampung burst (antrian) agar tidak langsung menolak serangan spasi singkat[18]. Bila memakai server Python (FastAPI), Anda juga bisa gunakan library seperti <code>slowapi</code> atau limit di layer kode.</li> <li>Autentikasi API Key: Lindungi endpoint publik dengan mekanisme kunci API atau token. Misalnya di FastAPI gunakan <code>APIKeyHeader</code> untuk mengecek header khusus, seperti contoh dokumentasi FastAPI:     <pre><code>from fastapi import Depends, FastAPI  \nfrom fastapi.security import APIKeyHeader  \n\napi_key_scheme = APIKeyHeader(name=\"X-API-Key\")  \n@app.get(\"/secure-data/\")  \nasync def read_secure(key: str = Depends(api_key_scheme)):  \nreturn {\"status\": \"OK\"}\n</code></pre>     FastAPI akan menolak jika header <code>X-API-Key</code> tidak benar[19]. Anda dapat memeriksa nilai kunci tersebut dengan logika tersendiri (misal mencocokkan dengan string yang disimpan di server). Alternatifnya, Anda bisa menerapkan validasi di layer NGINX (mis. memeriksa header/parameter kunci API) atau menggunakan OAuth 2.0/Bearer token untuk proteksi lebih lanjut.</li> <li>Pemantauan dan Logging: Aktifkan logging dan pantau performa. Gunakan sistem monitoring seperti Prometheus + Grafana untuk metrik. Jika pakai TGI, sudah terdapat dukungan metrik Prometheus dan tracing out-of-the-box[10]. Anda juga bisa menggunakan APM atau tools observabilitas (contoh: Sentry, OpenTelemetry) agar mengetahui error atau bottleneck. Pantau penggunaan GPU (mis. <code>nvidia-smi</code> atau <code>dcgm-exporter</code>), konsumsi RAM, dan statistik request (melalui NGINX access log atau FastAPI <code>logging</code>).</li> <li>Tips Lain: Terapkan CORS jika perlu (FastAPI: middleware CORS), batasi ukuran input prompt, dan jalankan container/service dengan user non-root. Perhatikan pembaruan keamanan pada OS dan library.</li> </ul> <p>Sumber: Referensi berasal dari dokumentasi resmi Hugging Face, Meta LLaMA\u00a03, dan sumber terpercaya terkait deployment LLM[1][6][7][10][12][13][17][19][15][16].</p> <p>[1] [2] [3] Installation</p> <p>https://huggingface.co/docs/transformers/en/installation</p> <p>[4] [5] [9] Installation from source</p> <p>https://huggingface.co/docs/text-generation-inference/en/installation</p> <p>[6] [7] GitHub - meta-llama/llama3: The official Meta Llama 3 GitHub site</p> <p>https://github.com/meta-llama/llama3</p> <p>[8] [10] Text Generation Inference</p> <p>https://huggingface.co/docs/text-generation-inference/en/index</p> <p>[11] GitHub - ggml-org/llama.cpp: LLM inference in C/C++</p> <p>https://github.com/ggml-org/llama.cpp</p> <p>[12] OpenAI Compatible Web Server - llama-cpp-python</p> <p>https://llama-cpp-python.readthedocs.io/en/latest/server/</p> <p>[13] [14] Setting Up a FastAPI Project with NGINX Reverse Proxy on Ubuntu - DEV Community</p> <p>https://dev.to/udara_dananjaya/setting-up-a-fastapi-project-with-nginx-reverse-proxy-on-ubuntu-883</p> <p>[15] Using ngrok to test local websites and APIs | Alex Hyett</p> <p>https://www.alexhyett.com/using-ngrok-to-test-local-sites-api/</p> <p>[16] Securely Exposing a Local Nginx Server with Cloudflare Tunnel - Gogola Nexus</p> <p>https://sebastiangogola.me/securely-exposing-a-local-nginx-server-with-cloudflare-tunnel/</p> <p>[17] [18] Rate Limiting with NGINX \u2013 NGINX Community Blog</p> <p>https://blog.nginx.org/blog/rate-limiting-nginx</p> <p>[19] Security Tools - FastAPI</p> <p>https://fastapi.tiangolo.com/reference/security/</p>"},{"location":"owasp/","title":"LLMs OWASP Introduction","text":""},{"location":"owasp/#apa-itu-owasp","title":"Apa itu OWASP?","text":"<p>OWASP (Open Worldwide Application Security Project) adalah organisasi global nirlaba yang fokus pada keamanan aplikasi. OWASP terkenal dengan daftar OWASP Top 10 untuk aplikasi web, yang membantu developer memahami risiko keamanan utama. Sekarang, OWASP juga mengeluarkan Top 10 untuk LLMs (Large Language Models), karena LLMs membawa risiko keamanan baru yang unik.</p>"},{"location":"owasp/#kenapa-llms-perlu-standar-keamanan","title":"Kenapa LLMs Perlu Standar Keamanan?","text":"<p>LLM (Large Language Model) memerlukan standar keamanan karena membawa risiko yang unik dan ganda: risiko keamanan siber tradisional (seperti kebocoran data) dan risiko keselamatan baru yang melekat pada kemampuan generatif (seperti penyalahgunaan etika dan penyebaran disinformasi). Standar keamanan diperlukan untuk mengelola kerentanan model, memastikan keandalan, dan membangun kepercayaan terhadap teknologi yang semakin dominan ini.</p>"},{"location":"owasp/#owasp-top-10-untuk-llms-2025","title":"OWASP Top 10 untuk LLMs (2025)","text":"<p>LLM adalah target baru bagi peretas karena llm berinteraksi langsung dengan data dan sistem. Standar diperlukan untuk mengatasi:</p> <ul> <li> <p>Prompt Injection: Serangan dengan \u201cmenyuntikkan\u201d instruksi tersembunyi ke dalam prompt agar model melakukan tindakan di luar batas yang diinginkan, misalnya membocorkan data atau mengeksekusi perintah berbahaya.</p> </li> <li> <p>Sensitive Information Disclosure: LLM bisa tanpa sengaja membocorkan data sensitif seperti API keys, data pelanggan, atau informasi internal perusahaan.</p> </li> <li> <p>Supply Chain Risks: LLM sering bergantung pada data, plugin, dan library pihak ketiga. Jika salah satu tidak aman, maka sistem bisa tereksploitasi.</p> </li> <li> <p>Data &amp; Model Poisoning: Hacker bisa menyuntikkan data berbahaya ke dataset training sehingga model bias atau menghasilkan output salah.</p> </li> <li> <p>Improper Output Handling: Output dari LLM bisa berbahaya jika langsung dijalankan (misalnya sebagai query SQL atau perintah shell).</p> </li> <li> <p>Excessive Agency: LLM yang diberi terlalu banyak kontrol (misalnya mengirim email, melakukan transfer uang, atau mengakses API) bisa salah digunakan.</p> </li> <li> <p>System Prompt Leakage: Instruksi sistem (system prompt) yang seharusnya rahasia bisa dibocorkan melalui trik tertentu.</p> </li> <li> <p>Vector &amp; Embedding Weaknesses: Sistem RAG (Retrieval-Augmented Generation) dan embedding bisa dimanipulasi dengan data berbahaya, sehingga hasil LLM tidak akurat atau berisiko.</p> </li> <li> <p>Misinformation (Halusinasi &amp; Disinformasi): LLM bisa memberikan jawaban yang salah, bias, atau bahkan dimanipulasi untuk menyebarkan hoaks.</p> </li> <li> <p>Unbounded Consumption: LLM dapat dikonsumsi secara berlebihan (misalnya input panjang, permintaan masif), yang bisa menyebabkan DoS (Denial of Service) atau biaya cloud yang membengkak.</p> </li> </ul>"},{"location":"owasp/data-model-poisoning/","title":"Data &amp; Model Poisoning","text":""},{"location":"owasp/excessive-agency/","title":"Excessive Agency","text":""},{"location":"owasp/improper-output-handling/","title":"Improper Output Handling","text":""},{"location":"owasp/misinformation/","title":"Misinformation (Halusinasi &amp; Disinformasi)","text":""},{"location":"owasp/prompt-injection/","title":"Prompt Injection","text":""},{"location":"owasp/prompt-injection/#description","title":"Description","text":"<p>Kerentanan Prompt Injection terjadi ketika prompt mengubah perilaku atau output LLM dengan cara yang tidak diinginkan. Input tersebut dapat memengaruhi model meskipun tidak terlihat oleh manusia, oleh karena itu prompt injection tidak perlu terlihat/dapat dibaca oleh manusia, asalkan kontennya diproses oleh model.</p> <p>Kerentanan prompt injection terjadi ketika model dipaksa oleh input agar salah memproses instruksi \u2014 misalnya instruksi tersembunyi dari pengguna bisa membuat model melanggar kebijakan, menghasilkan konten berbahaya, memberi akses tanpa izin, atau mempengaruhi keputusan penting. Meskipun teknik seperti Retrieval Augmented Generation (RAG) dan fine-tuning digunakan agar keluaran LLM lebih relevan dan akurat, penelitian menunjukkan bahwa teknik ini tidak bisa sepenuhnya menghentikan serangan prompt injection.</p> <p>Istilah prompt injection dan jailbreaking sering digunakan saling bergantian, tapi memiliki makna berbeda. Prompt injection adalah manipulasi dengan memasukkan input khusus untuk mengubah perilaku model, termasuk melewati mekanisme keamanan. Jailbreaking adalah jenis ekstrem dari prompt injection di mana penyerang membuat model mengabaikan seluruh protokol keamanan. Untuk melindungi sistem, pengembang bisa menanamkan batasan pada prompt sistem dan menangani input dengan hati-hati. Namun agar jailbreaking benar-benar dicegah, perlu pembaruan terus-menerus pada training dan mekanisme keamanan model.</p>"},{"location":"owasp/prompt-injection/#types-of-prompt-injection","title":"types of Prompt Injection","text":"<ul> <li> <p>Direct Prompt Injections,  terjadi ketika prompt input secara langsung mengubah perilaku model dengan cara yang tidak diinginkan atau tidak terduga. Input tersebut dapat bersifat disengaja (misalnya, aktor jahat yang sengaja merancang prompt untuk memanipulasi model) atau tidak disengaja (misalnya, pengguna secara tidak sengaja memberikan input yang memicu perilaku yang tidak terduga).</p> </li> <li> <p>Indirect Prompt Injections, terjadi ketika Model Bahasa Besar (LLM) mengambil input dari sumber eksternal \u2014 seperti website, email, dokumen, atau file lain \u2014 yang mengandung instruksi tersembunyi. Instruksi ini bisa membuat model bertindak cara yang tidak diinginkan ketika model memproses konten tersebut. Dampaknya bisa beragam tergantung konteks bisnis dan bagaimana model dirancang. Beberapa risiko umum adalah:</p> <ul> <li>Bocornya data sensitif</li> <li>Pengungkapan informasi infrastruktur sistem AI</li> <li>Hasil output yang salah atau bias</li> <li>Akses tidak sah ke fungsi tertentu</li> <li>Perintah arbitrer dijalankan di sistem yang terhubung</li> <li>Proses pengambilan keputusan penting terpengaruh</li> </ul> <p>Dengan makin berkembangnya AI multimodal (yang memproses teks, gambar, dsb. bersamaan), muncul risiko baru seperti instruksi tersembunyi di dalam gambar atau interaksi antar modality yang sulit dideteksi. Penanganannya butuh penelitian dan pertahanan khusus.</p> </li> </ul>"},{"location":"owasp/prompt-injection/#prevention-and-mitigation-strategies","title":"Prevention and Mitigation Strategies","text":"<p>Kerentanan prompt injection dapat terjadi karena sifat AI generatif. Mengingat pengaruh stochastic yang mendasari cara kerja model, belum jelas apakah ada metode pencegahan yang sepenuhnya aman untuk prompt injection. Namun, langkah-langkah berikut dapat mengurangi dampak prompt injection:</p> <ol> <li> <p>Constrain model behavior</p> <p>Berikan instruksi spesifik mengenai peran, kemampuan, dan batasan model dalam prompt sistem. Pastikan kepatuhan ketat terhadap konteks, batasi respons pada tugas atau topik tertentu, dan instruksikan model untuk mengabaikan upaya untuk mengubah instruksi inti.</p> </li> <li> <p>Define and validate expected output formats</p> <p>Tentukan format output yang jelas, minta penjelasan rinci dan kutipan sumber, dan gunakan kode deterministik untuk memvalidasi kepatuhan terhadap format-format tersebut.</p> </li> <li> <p>Implement input and output filtering</p> <p>Tentukan kategori sensitif dan buat aturan untuk mengidentifikasi dan menangani konten semacam hal tersebut. Terapkan semantik filter dan string-checking untuk memindai konten yang tidak diizinkan. Evaluasi respons menggunakan RAG Triad: Evaluasi relevansi konteks, keandalan, dan relevansi pertanyaan/jawaban untuk mengidentifikasi output yang berpotensi berbahaya.</p> <p></p> </li> <li> <p>Enforce privilege control and least privilege access</p> <p>Berikan aplikasi dengan token API-nya sendiri untuk fungsionalitas yang dapat diperluas, dan tangani fungsi-fungsi tersebut dalam kode daripada memberikannya kepada model. Batasi hak akses model hanya pada yang diperlukan untuk operasi yang dimaksudkan.</p> </li> <li> <p>Require human approval for high-risk actions</p> <p>Implementasikan human-in-the-loop control (keterlibatan kontrol manusia) untuk operasi yang memerlukan izin khusus guna mencegah tindakan yang tidak sah.</p> </li> <li> <p>Segregate and identify external content</p> <p>Pisahkan dan tandai dengan jelas konten yang tidak tepercaya untuk membatasi pengaruhnya terhadap prompt pengguna.</p> </li> <li> <p>Conduct adversarial testing and attack simulations</p> <p>Lakukan penetration testing dan breach simulations secara rutin, dengan memperlakukan model sebagai pengguna yang tidak tepercaya untuk menguji efektivitas batas kepercayaan dan kontrol akses.</p> </li> </ol>"},{"location":"owasp/prompt-injection/#example-attack-scenarios","title":"Example Attack Scenarios","text":"<ol> <li> <p>Direct Injection</p> <p>Seorang penyerang menyisipkan perintah ke dalam chatbot layanan pelanggan, memerintahkannya untuk mengabaikan pedoman, mengakses basis data pribadi, dan mengirim email, yang mengakibatkan akses tidak sah dan eskalasi hak akses.</p> </li> <li> <p>Indirect Injection</p> <p>Seorang pengguna menggunakan LLM untuk merangkum sebuah halaman web yang mengandung instruksi tersembunyi yang menyebabkan LLM menyisipkan gambar yang mengarah ke URL, yang mengakibatkan kebocoran data pribadi.</p> </li> <li> <p>Unintentional Injection</p> <p>Seorang pelamar menggunakan LLM untuk menyempurnakan resume-nya \u2014 dan secara tidak sengaja memicu sinyal bahwa resume itu dibuat oleh AI sesuai kebijakan perusahaan.</p> </li> <li> <p>Intentional Model Influence</p> <p>Seorang penyerang memodifikasi dokumen dalam repositori yang digunakan aplikasi untuk Retrieval-Augmented Generation (RAG). Ketika query mengembalikan konten yang dimodifikasi, instruksi berbahaya tersebut mengubah output LLM, menghasilkan hasil yang menyesatkan.</p> </li> </ol>"},{"location":"owasp/sensitive-information-disclosure/","title":"Sensitive Information Disclosure","text":""},{"location":"owasp/supply-chain-risks/","title":"Supply Chain Risks","text":""},{"location":"owasp/system-prompt-leakage/","title":"System Prompt Leakage","text":""},{"location":"owasp/unbounded-consumption/","title":"Unbounded Consumption","text":""},{"location":"owasp/vector-embedding-weaknesses/","title":"Vector &amp; Embedding Weaknesses","text":""},{"location":"tech-stack/","title":"Framework","text":""},{"location":"tech-stack/#framework-yang-populer-di-2025","title":"Framework yang populer di 2025","text":"Nama Pros / Kelebihan Kekurangan / Tantangan Biaya / Model Harga Peruntukan / Kasus penggunaan cocok LangChain - Sangat populer, komunitas besar, banyak contoh &amp; integrasi (LLM, vector DB, tool external). (AI Agents for Smart Automation)  - Fitur chaining prompt, memory, tools, agent, RAG etc.  - Ada ekosistem (LangSmith untuk observability, LangGraph untuk orkestrasi agent / deployment stateful) (LangChain) - Bisa jadi kompleks (kompleksitas bertambah cepat) khususnya untuk workflow agent/graph besar.  - Biaya luar biasa jika penggunaan tinggi (banyak panggilan ke API model / penyimpanan vector / hosting / tracing).  - Ketergantungan ke layanan eksternal &amp; LLM komersial \u2192 bisa mahal + isu privasi jika data sensitif.  - Beberapa breaking changes di library bisa menyulitkan maintenance. (Reddit) - Framework inti (library) bersifat open source / MIT licence \u2192 bisa dipakai bebas tanpa biaya lisensi. (educative.io)  - LangSmith (observability/tracing) punya tier gratis &amp; berbayar. Gratis untuk 1 user + beberapa traces, setelah itu bayar per unit trace atau seat. (docs.smith.langchain.com)  - Untuk deployment production, hosting, infrastruktur model, penyimpanan vector, API LLM = biaya tambahan signifikan (tergantung model &amp; jumlah permintaan). Prototyping, aplikasi agent + RAG, chatbot, Q&amp;A internal, asisten pintar, proses otomatisasi. Cocok untuk startup sampai enterprise yang butuh fleksibilitas &amp; integrasi banyak service. Rasa - Open-source, memungkinkan self-hosting sehingga kontrol penuh atas data &amp; privasi. (Rasa)  - Fokus bagus pada dialog / conversational flow klasik, tersedia tools untuk NLU, dialog management, integrasi ke channel (chat, voice) dll.  - Fitur enterprise tersedia (scalability, observability, keamanan) di versi berbayar. (Rasa) - Kurang fokus pada LLM generatif / agent autonomus multi-tool dibanding LangChain; jika ingin agent memakai banyak tool &amp; reasoning sangat kompleks, mungkin perlu banyak modifikasi.  - Setup awal bisa lebih rumit, terutama jika kebutuhan dialog sangat kompleks + banyak integrasi.  - Biaya terutama di versi enterprise / scale besar cukup tinggi. - Ada versi \u201cDeveloper Edition\u201d gratis / open source untuk pengembangan &amp; produksi kecil. (Rasa)  - Untuk skala pertumbuhan (\u201cGrowth\u201d) dan model enterprise, mulai dari sekitar USD 35.000/tahun untuk &gt; 500.000 percakapan/tahun plus dukungan dan fitur -- angka ini bisa sangat berubah tergantung jumlah percakapan, kebutuhan SLA, keamanan. (Rasa) Cocok untuk chatbot, asisten percakapan di perusahaan, customer service, sistem dialog interaktif, voice agents. Terutama jika butuh kontrol kuat atas data &amp; alur percakapan. Semantic Kernel (Microsoft) - Struktur konsep yang jelas: skills, memories, planner dll. Cocok jika kamu mau agent modular, dengan banyak fungsi &amp; kolaborasi antar-agent. (AI Agents for Smart Automation)  - Integrasi baik dengan .NET / ekosistem Microsoft / Azure. Jika kamu sudah menggunakan stack Microsoft, bisa sangat cocok.  - GA (General Availability) untuk fitur agent sudah tersedia. (Microsoft for Developers) - Jika kamu di luar ekosistem Microsoft, mungkin relatif kurang dokumentasi / komunitas dibanding LangChain.  Beberapa fitur mungkin masih baru &amp; butuh eksperimen.  Ketergantungan ke layanan Azure jika ingin manfaat penuh \u2192 biaya bisa tinggi &amp; ada isu lock-in. - Banyak bagian inti tersedia gratis / open source (terutama SDK).  Tapi jika memakai layanan Azure seperti Azure OpenAI, Azure storage, dll, maka biaya akan tergantung penggunaan cloud tersebut.  Detail harga spesifik tergantung region &amp; layanan yang digunakan. Cocok untuk perusahaan yang menggunakan Microsoft Azure, ingin agen modular, internal business logic, automatisasi proses, integrasi enterprise, penerapan di lingkungan .NET / Windows / Azure. AutoGPT - Fokus pada agen yang sangat otomatis / self-driven: tujuan \u2192 memecah ke sub-tugas \u2192 eksekusi otomatis. Praktis untuk eksperimen &amp; tugas otomatis yang tidak perlu interaksi manusia terus menerus. (Wikipedia)  - Open source. - Stabilitas masih menjadi masalah: bisa loop, hallucination, error handling sulit.  Kurang kontrol &amp; observability / debug dibanding framework yang lebih matang.  Untuk produksi / tugas kritis, perlu banyak pengamanan &amp; pengujian.  Biaya terkait model &amp; infrastruktur bisa tinggi jika usage besar. - Gratis untuk framework / kode dasar.  Biaya muncul dari penggunaan model-LLM (API) dan hosting.  Jika kamu pakai LLM berbayar, atau memakai instance lokal yang resource berat, itu biaya signifikan. Eksperimen, proof-of-concept, automatisasi tugas back-end, agent pribadi, proyek riset, prototyping. Kurang cocok langsung produksi skala besar tanpa tambahan sistem pendukung. CrewAI - Lightweight, fokus pada kolaborasi antar agent (multi-agent workflows). (AI Agents for Smart Automation)  - Cocok jika kamu punya tugas yang bisa dibagi antar sub-role agent (writer, researcher, analyst, dll).  Bisa lebih cepat mulai jika dibanding bangun sistem multi-agent dari nol. - Masih baru; dokumentasi / contoh / stabilitasnya mungkin kurang dibanding yang lebih mature.  Fitur mungkin belum lengkap; debugging &amp; monitoring mungkin belum sebaik framework mapan. - Karena baru, banyak bagian masih gratis / open source; biaya muncul jika kamu ingin dukungan enterprise, integrasi khusus, atau hosting pada skalanya.  Tidak ada banyak data publik tentang harga sudah pasti. Kolaborasi tim, tugas-multi-peran, content creation, riset, proyek kreatif yang butuh agent berbeda untuk bagian berbeda. LlamaIndex (sebelumnya GPT Index) - Fokus pada pengindeksan dan RAG (Retriever-Augmented Generation). Bagus untuk aplikasi yang butuh pencarian dokumen/pengambilan informasi dari banyak sumber (PDF, web, database). (AI Agents for Smart Automation)  - Bisa dikombinasikan dengan LangChain &amp; framework lainnya. - Tidak fokus pada agent orchestration / multi-agent / tool integration seperti LangChain; lebih sebagai bagian dari pipeline RAG.  Untuk agent yang butuh reasoning kompleks / tindakan/aksi, perlu digabung dengan komponen lain. - Banyak bagian open source gratis.  Biaya seperti biasa: hosting data, penyimpanan vector, query DB, penggunaan model LLM jika dipakai. Aplikasi seperti search internal / knowledge base, asistensi internal berdasarkan dokumen, Q&amp;A berbasis konteks dokumen. MetaGPT - Dirancang untuk simulasi tim pengembangan perangkat lunak (CEO, PM, developer agent) \u2192 bisa otomatisasi tugas proyek software, manajemen proyek, code generation. (Medium)  - Bagus untuk prototyping proyek software yang kompleks di mana agent bisa dibagi role. - Relatif domain sempit (fokus pada pengembangan software) dibanding framework yang lebih general.  Mungkin tidak fleksibel jika tugas sangat berbeda dari skenario pengembangan software.  Dokumentasi / ekosistem belum sebesar LangChain dsb. - Umumnya open source / gratis untuk penggunaan dasar / riset. Biaya muncul dari infrastruktur / penggunaan model eksternal etc. Proyek software development automation, koordinasi tim (agent per role), generasi kode, dokumentasi &amp; tugas PD jika tim dev ingin otomatisasi bagian dari workflownya. LangGraph - Graph-based workflows; memungkinkan proses yang kompleks &amp; stateful, orkestrasi agent, manajemen state &amp; memory dengan kontrol yang bagus. (BestAIAgents)  - Mendukung human-in-the-loop, debugging, pemantauan node workflows dll. - Belajar / setting awal bisa sulit jika belum terbiasa dengan konsep graph / node state.  Kompleksitas &amp; overhead operasional bisa tinggi.  Untuk proyek sederhana mungkin overkill. - Sering bagian dari LangChain / LangSmith + LangGraph ekosistem; biaya terkait trace / seat / deployment seperti LangChain.  Beberapa fitur gratis, tapi untuk tim &amp; produksi, akan ada biaya tambahan. Cocok untuk workflows agent kompleks, pipeline multi-tahap, aplikasi di mana agent perlu bekerja dalam urutan kondisi, branching, stateful conversations dll. AgentGPT / AgentGPT-type UIs - UI friendly; bagus untuk cepat bikin agent eksploratif / eksperimen  Memungkinkan user non-teknis untuk menyetel tujuan dan melihat / mengontrol agent.  Bisa bagus untuk demo / produk minimum viable. - Kurang fleksibilitas dibanding coding langsung; mungkin tidak cocok jika butuh integrasi mendalam atau custom tool / logika kompleks.  Masih bergantung ke layanan eksternal / LLM berbayar.  Kemungkinan masalah keamanan / privasi jika agent UI mengirim data ke cloud. - Bagian UI / host biasanya gratis atau memiliki model freemium; tetapi penggunaan model / API eksternal dan penyimpanan / infrastruktur tetap ada biayanya. Prototipe, demo, solusi cepat, user-facing agent ringan, startup yang ingin MVP cepat."},{"location":"tech-stack/commercial-b2b/","title":"Commercial B2B License","text":"<p>Berikut versi tabel lisensi dan konsolidasi LLM provider untuk produk komersial (SaaS publik / B2B)</p> <p>Tabel ini membantu Anda menentukan kapan perlu melakukan konsolidasi atau perjanjian komersial dengan penyedia model, serta apa risikonya jika tidak.</p>"},{"location":"tech-stack/commercial-b2b/#tabel-ketentuan-penggunaan-komersial-model-llm-api-on-premise","title":"\ud83e\udde0 Tabel: Ketentuan Penggunaan Komersial Model LLM (API &amp; On-Premise)","text":"Provider / Model Kepemilikan Output Boleh untuk SaaS / B2B Komersial? Monetisasi Output Perlu Konsolidasi / Lisensi Enterprise? Risiko / Catatan Hukum &amp; Kepatuhan OpenAI (GPT-3.5 / GPT-4 / o1) Pengguna memiliki semua hak atas output. Data API tidak digunakan untuk pelatihan model (kecuali ChatGPT gratis).(openai) (openai) \u2705 Diperbolehkan membuat aplikasi publik atau B2B (openai). Tidak boleh menjual ulang API key (openai). \u2705 Ya, bebas komersial. \u2699\ufe0f Tidak wajib, tapi disarankan untuk Enterprise jika memproses data sensitif (DPA tersedia). Pelanggaran Terms (mis. sharing key, bypass rate limit) dapat menyebabkan pemblokiran akun. Tanpa DPA \u2192 risiko privasi data di sisi hukum GDPR/HIPAA. Anthropic (Claude 3 series) Anda memiliki hak penuh atas input dan output (anthropic). \u2705 Diperbolehkan untuk produk SaaS dan B2B (anthropic). \u2705 Ya, termasuk produk berbayar (anthropic). \u2699\ufe0f Tidak wajib, tapi disarankan untuk enterprise (Claude for Work/Enterprise) (anthropic). Anthropic tidak melatih model dengan data Anda, namun tetap tunduk pada Usage Policy (anthropic). Google (Gemini / Vertex AI) Pengguna memiliki output, tapi Google dapat menghasilkan konten serupa untuk orang lain (google). \u2705 Diperbolehkan, tapi wilayah dibatasi (region-based). Pengguna EEA/UK wajib pakai versi berbayar (Google Cloud) (google). \u2705 Ya. \u2699\ufe0f Disarankan pakai Google Cloud Enterprise untuk produk B2B. Layanan gratis bisa menggunakan data Anda untuk pelatihan (google). Gunakan versi berbayar agar data tidak disimpan (google). Mistral AI (Mixtral, Mistral 7B, Codestral) Pengguna memiliki hak penuh atas output (mistral). \u2705 Diperbolehkan (mistral) (mistral). \u2705 Ya. \u2699\ufe0f Tidak wajib, tapi untuk data sensitif disarankan aktifkan Zero Data Retention (ZDR) (mistral). Tanpa ZDR, data bisa disimpan sementara di log. Tidak digunakan untuk training, tapi raw log tetap ada. LLaMA (Meta LLaMA 2 / 3) Model open-weight (OSS). Meta tidak mengklaim output. \u26a0\ufe0f Perlu izin lisensi komersial untuk aplikasi dengan lebih dari 700 juta MAU atau digunakan di platform besar. \u2705 Ya, tapi tunduk pada LLaMA 2/3 Commercial License. \u2699\ufe0f Jika aplikasi Anda berskala besar atau revenue-driven, harus mendaftar lisensi komersial Meta (gratis untuk sebagian besar startup). Pelanggaran lisensi (misal penggunaan tanpa izin pada skala besar) dapat mengakibatkan klaim pelanggaran lisensi. Jika dimodifikasi dan dijual ulang, wajib mencantumkan atribusi Meta. DeepSeek (R1, Coder, Chat) Open-weight model (MIT License atau Apache 2.0 tergantung distribusi). Pengguna memiliki hak output. \u2705 Diperbolehkan. Tidak ada batasan eksplisit untuk SaaS/B2B. \u2705 Ya, bebas komersial karena lisensi Apache 2.0 memperbolehkan penggunaan komersial tanpa izin tambahan. \u2699\ufe0f Tidak perlu konsolidasi \u2014 cukup mencantumkan atribusi jika diwajibkan oleh lisensi (mis. Apache 2.0 Notice). Pastikan tidak melanggar lisensi dependensi (mis. data training publik). DeepSeek menyatakan tidak menyimpan data pengguna, tapi tanggung jawab tetap di pihak developer untuk data privacy (GDPR/HIPAA). LlamaIndex / LangChain (Integrator Layer) Framework OSS (MIT License). Tidak menyimpan data pengguna secara otomatis. \u2705 Diperbolehkan digunakan untuk produk SaaS &amp; B2B. \u2705 Ya. \u274c Tidak perlu lisensi komersial. Pastikan Anda mematuhi lisensi model dan penyedia API yang diintegrasikan melalui framework ini. OpenRouter / LiteLLM (Aggregator Layer) Tidak memproses isi untuk pelatihan. Output tetap milik pengguna. \u2705 Diperbolehkan, tapi mengikuti syarat tiap model di dalamnya. \u2705 Ya. \u2699\ufe0f Tidak perlu lisensi khusus, tapi Anda bertanggung jawab atas semua model yang digunakan. Risiko: jika salah satu model melarang redistribusi output, Anda tetap bisa dituntut. Pastikan Terms tiap model terpenuhi."},{"location":"tech-stack/commercial-b2b/#kesimpulan-rekomendasi","title":"\u2699\ufe0f Kesimpulan &amp; Rekomendasi","text":"Situasi Perlu Konsolidasi / Perjanjian Komersial? Catatan Startup SaaS publik (skala kecil-menengah, &lt;1M MAU) \u274c Tidak wajib Gunakan API publik (OpenAI, Mistral, Claude) sesuai terms. Pastikan data sensitif dienkripsi. B2B Enterprise (kontrak dengan perusahaan besar) \u2699\ufe0f Disarankan Lakukan Data Processing Agreement (DPA) dan dapatkan Enterprise Terms untuk SLA dan kepatuhan (ISO, SOC2, dsb). Penggunaan model OSS (LLaMA, DeepSeek, Mistral local) \u274c Tidak wajib, tapi \u26a0\ufe0f cek lisensi modelnya LLaMA memerlukan lisensi komersial untuk platform besar; DeepSeek bebas komersial. Penggunaan multi-model router (LiteLLM, OpenRouter) \u2699\ufe0f Pastikan setiap model sesuai Terms-nya Router tidak menghapus tanggung jawab legal Anda terhadap Terms tiap API. Produk dengan data sensitif (HR, medis, finansial) \u2705 Wajib DPA / Enterprise SLA Hindari API publik tanpa perjanjian; gunakan versi enterprise (mis. OpenAI Enterprise, Claude for Work)."},{"location":"tech-stack/cost-simulation/","title":"Simulasi Biaya","text":"<p>Berikut simulasi estimasi biaya ($) bulanan untuk penggunaan LLM, dengan asumsi:</p> <ul> <li>MAU: 500 pengguna aktif per bulan</li> <li>Sesi/per bulan per pengguna: 10 sesi</li> <li>Token per sesi: input 200 token + output 400 token = 600 total token/sesi</li> <li>Total sesi per bulan: 500 \u00d7 10 = 5.000 sesi</li> </ul> <p>Jadi total token per bulan = 5.000 sesi \u00d7 600 = 3.000.000 token (input + output).</p> <p>Kita gunakan asumsi model hybrid: memakai model murah untuk sebagian besar tugas rutin, dan model lebih kuat/premium untuk tugas khusus (misal final summary/feedback resmi). Saya pilih 2 model murah dari data harga OpenRouter untuk simulasi:</p> <ul> <li>Mistral Medium 3.1: Input $0.40 / 1M token; Output $2.00 / 1M token. (Privacy AI)</li> <li>DeepSeek R1: Input $0.40 / 1M token; Output $2.00 / 1M token. (Privacy AI)</li> <li>GPT-4o: sebagai model premium dipakai hanya untuk final summary &amp; feedback resmi. Harga: Input $2.50 / 1M token; Output $10.00 / 1M token. (Economize Cloud)</li> </ul>"},{"location":"tech-stack/cost-simulation/#perhitungan-biaya-untuk-skenario-hybrid","title":"\ud83e\uddee Perhitungan Biaya untuk Skenario Hybrid","text":"<p>Kita asumsikan:</p> <ul> <li>80% dari sesi memakai model murah (Mistral atau DeepSeek) untuk tugas otomatis/harian (coaching, ringkasan kecil, klasifikasi, sentiment, draft feedback)</li> <li>20% dari sesi memakai GPT-4o untuk final summary / feedback resmi / dokumen akhir</li> </ul> <p>Jadi:</p> <ul> <li>Sesi model murah = 0,80 \u00d7 5.000 = 4.000 sesi</li> <li>Sesi model premium = 0,20 \u00d7 5.000 = 1.000 sesi</li> </ul>"},{"location":"tech-stack/cost-simulation/#biaya-per-sesi-model","title":"Biaya per sesi / model","text":"<ul> <li> <p>Model murah (contoh Mistral Medium 3.1 atau DeepSeek R1 \u2013 harga input $0.40 / 1M + output $2.00 / 1M):</p> </li> <li> <p>Token per sesi = 600 token</p> </li> <li>Biaya input = 200 token \u00d7 ($0.40 / 1.000.000) = $0.00008</li> <li>Biaya output = 400 token \u00d7 ($2.00 / 1.000.000) = $0.00080</li> <li> <p>Total biaya per sesi \u2248 $0.00088</p> </li> <li> <p>Model premium (GPT-4o \u2013 input $2.50 / 1M + output $10.00 / 1M):</p> </li> <li> <p>Biaya input = 200 \u00d7 ($2.50 / 1.000.000) = $0.00050</p> </li> <li>Biaya output = 400 \u00d7 ($10.00 / 1.000.000) = $0.00400</li> <li>Total biaya per sesi \u2248 $0.00450</li> </ul>"},{"location":"tech-stack/cost-simulation/#biaya-bulanan","title":"Biaya bulanan","text":"<ul> <li>Model murah: 4.000 sesi \u00d7 $0.00088 = $3.52</li> <li>Model premium: 1.000 sesi \u00d7 $0.00450 = $4.50</li> </ul> <p>Total estimasi biaya LLM per bulan (token usage) \u2248 $3.52 + $4.50 = $8.02</p>"},{"location":"tech-stack/cost-simulation/#catatan-dan-faktor-tambahan","title":"\u26a0\ufe0f Catatan dan Faktor Tambahan","text":"<ul> <li>Ini hanya biaya token / inference; belum termasuk biaya infrastruktur lain seperti server backend, hosting, database, storage, monitoring, keamanan.</li> <li>Tidak termasuk potensi kuota minimum, biaya penyimpanan data, biaya perangkat keras jika self-host, dan biaya lisensi atau DPA.</li> <li>Token penggunaan mungkin lebih besar jika ada konteks panjang, prompt system yang kompleks, atau output panjang dalam summary atau dokumen resmi.</li> <li>Model \u201cmurah\u201d mungkin mempunyai latency lebih tinggi atau kualitas yang sedikit lebih rendah dibanding premium, sehingga untuk tugas penting premium lebih disukai.</li> </ul>"},{"location":"tech-stack/dev-requirement/","title":"Development Requirement","text":""},{"location":"tech-stack/dev-requirement/#llms-models","title":"LLMs Models","text":""},{"location":"tech-stack/dev-requirement/#openrouter","title":"OpenRouter","text":"<p>Di bawah ini tabel ringkas tentang model-model LLM yang tersedia via OpenRouter yang layak dipertimbangkan. Model-model berikut yang sering dipakai karena keseimbangan akurasi, konteks panjang, biaya, dan kebijakan penggunaan. Semua angka harga &amp; info diambil dari katalog OpenRouter (lihat sitasinya di akhir tabel) \u2014 periksa kembali sebelum produksi karena harga/penawaran bisa berubah.</p> <p>Catatan : kolom Price mengacu pada biaya yang ditampilkan OpenRouter (biasanya $ per 1M token input / output atau rate serupa). Policy merujuk pada lisensi &amp; batasan penggunaan umum (provider asli + OpenRouter gateway). Untuk penggunaan data sensitif, utamakan model yang bisa dijalankan on-prem / self-hosted atau pastikan kontrak/data processing agreement (DPA) dari provider.</p>"},{"location":"tech-stack/dev-requirement/#tabel-model","title":"Tabel model","text":"Model (Provider) Kesesuaian (fit) Price Pros (kenapacocok) Cost considerations / Kekurangan Policy / Catatan kepatuhan Zero Data Retention gpt-4o-latest (OpenAI) (OpenRouter) Sangat cocok untuk fitur penulisan natural, ringkasan panjang, penjelasan kebijakan, dialogs coaching. Contoh: Input $5 / 1M token, Output $15 / 1M token (lihat model comparator). Harga premium. (OpenRouter) - Kualitas bahasa &amp; reasoning sangat tinggi.- Mendukung modalitas lanjutan &amp; context panjang. - Biaya per-token relatif tinggi \u2192 hitung budget untuk volume (sesi performance reviews panjang).- Latensi &amp; cost sensitif untuk real-time heavy usage. - Kebijakan OpenAI: data yang dikirim dapat dipakai untuk peningkatan model kecuali dinonaktifkan lewat enterprise / DPA; cek persyaratan kontrak. (OpenRouter) claude-3.7-sonnet / claude-family (Anthropic) (OpenRouter) Sangat cocok untuk long-form, safety, dan menolak permintaan berbahaya. Input $5 / 1M token, Output $15 / 1M token - Kuat pada safety/alignment \u2192 baik untuk kasus sensitif. - Kinerja bagus pada instruksi &amp; konsistensi. - Masih lebih mahal daripada model open-source; butuh anggaran jika volume besar. - Anthropic menekankan safety &amp; harmlessness; periksa syarat pemrosesan data dan opsi enterprise untuk DPA. (OpenRouter) Mistral Medium 3.1 / Mistral family (OpenRouter) Cocok untuk fitur sehari-hari: summarization, intent classification, templated responses (biaya/performansi seimbang). Input $0.4 / 1M token, Output $2 / 1M token - Rasio cost/performance sangat baik.- Baik untuk scaling (banyak organisasi memilih Mistral untuk cost efficiency). - Kurang \u201ccutting-edge\u201d dibanding GPT4o/Claude pada beberapa tugas kompleks; perlu prompt engineering. - Biasanya provider model open (atau commercial) punya kebijakan berbeda; jika pakai via OpenRouter, patuhi terms provider masing-masing. (OpenRouter) Llama 4 Maverick / Llama family (Meta/NVIDIA variants via OpenRouter) (OpenRouter) Bagus bila ingin self-hosting / fine-tune (jika tersedia on-prem) \u2014 ideal untuk data sensitif karyawan. Input $0.15 / 1M token, Output $0.6 / 1M token; self-hosting = biaya infra (GPU) bukan token. - Dapat di-fine-tune / quantize.- Kontrol penuh atas data (jika self-host). - Jika self-host: membutuhkan GPU (A100/H100) &amp; MLOps (CAPEX/OPEX). Via API: cek latency &amp; SLA. - Lisensi model (Meta/NVIDIA) wajib dicek; self-hosting memberi keuntungan kepatuhan data (DPA lebih mudah diterapkan). (OpenRouter) DeepSeek V3.1 (free tier / low-cost models) (OpenRouter) Pilihan ekonomi untuk prototyping dan fitur non-misi-kritis (summaries, suggestions). Input $0.2 / 1M token, Output $0.8 / 1M token - Biaya rendah / tersedia free tier \u2192 cepat untuk MVP.- Cukup untuk banyak tugas ringan. - Kualitas mungkin lebih rendah pada reasoning kompleks; periksa hallucination. - Model komunitas/third-party: periksa lisensi penggunaan komersial. Jika PII diolah, pastikan kebijakan penyimpanan/retensi. (OpenRouter) Qwen / Deepseek / Lain-lain (mis. Qwen-3, DeepSeek R1T Chimera, etc.) (compare-openrouter-models.pages.dev) Alternatif cost-effective untuk tugas klasifikasi, ekstraksi entitas, pembuatan template (OKR). Harga sangat bervariasi \u2014 beberapa model sangat murah per 1M token di OpenRouter. Gunakan comparator spreadsheet OpenRouter. (compare-openrouter-models.pages.dev) - Pilihan beragam \u2192 bisa mix-and-match (OpenRouter routing) untuk menyeimbangkan cost &amp; quality. - Variansi kualitas antara model; butuh A/B testing. - Periksa terms setiap provider; beberapa model komunitas memiliki batasan komersial. (compare-openrouter-models.pages.dev)"},{"location":"tech-stack/dev-requirement/#rekomendasi-pemilihan-strategi-biaya","title":"Rekomendasi pemilihan &amp; strategi biaya","text":"<ol> <li>Mix-and-match (hybrid): gunakan model premium (GPT-4o/Claude) untuk tugas berisiko tinggi (final summaries, legal wording), dan model mid/low-cost (Mistral, DeepSeek) untuk tugas otomatis/volume tinggi (intent detection, quick summaries). OpenRouter memudahkan switching antar-model lewat satu API. (OpenRouter)</li> <li>RAG + caching: untuk knowledge base perusahaan, gunakan RAG (ambil dokumen internal) sehingga model tidak perlu menghabiskan token pada knowledge static \u2014 menghemat biaya &amp; mengurangi hallucination. (Implementasi: embed + vector DB). (OpenRouter)</li> <li>Self-host untuk data sensitif: jika harus memproses PII/penilaian karyawan yang sensitif, pertimbangkan model self-host (Llama family) untuk strict data control\u2014tapi perhitungkan biaya GPU &amp; MLOps. (OpenRouter)</li> <li>Periksa policy &amp; DPA: selalu konfirmasi DPA/retention/penggunaan data dengan provider model (terutama OpenAI/Anthropic). OpenRouter bertindak sebagai gateway\u2014tetapi policy utama tetap dari provider model yang mengeksekusi inference. (OpenRouter)</li> </ol>"},{"location":"tech-stack/dev-requirement/#link-sumber-openrouter-silakan-klik-untuk-detail-angka-terbaru","title":"Link &amp; Sumber (OpenRouter \u2014 silakan klik untuk detail &amp; angka terbaru)","text":"<ul> <li>Halaman model &amp; browsing OpenRouter: OpenRouter Models. (OpenRouter)</li> <li>Perbandingan model &amp; pricing contoh (model comparator halaman): Model Comparison / pricing examples. (OpenRouter)</li> <li>Halaman utama OpenRouter (overview gateway, routing, policy reminder): openrouter.ai. (OpenRouter)</li> <li>Contoh listing model free / low-cost (DeepSeek etc.): OpenRouter models free. (OpenRouter)</li> </ul>"},{"location":"tech-stack/dev-requirement/#frameworks-tools","title":"Frameworks &amp; Tools","text":"Lapisan / Komponen Teknologi / Framework Fungsi Utama Alternatif / Integrasi Kelebihan (Pros) Catatan / Cost / Kapan Digunakan 1. Orkestrasi AI Agent LangGraph Mengatur workflow agent berbasis graph (node-state-edge). Mendukung alur multi-langkah, kondisi bercabang, dan kolaborasi multi-agent. CrewAI, AutoGen, Semantic Kernel + Visual &amp; modular+ Mendukung loop &amp; persistence+ Integrasi langsung dengan LangChain Gratis (OSS). Ideal untuk AI Agent kompleks seperti HR performance management dengan multi-sesi dan analisis data dinamis. 2. LLM Core Orchestration LangChain Core Abstraksi modular untuk prompt, model, memory, tools, dan chain. Menjadi inti logika AI agent. LlamaIndex, Haystack + Sangat fleksibel &amp; ekosistem besar+ Integrasi native ke LiteLLM, LangGraph, LangSmith Gratis (OSS). Sebagai engine utama agent reasoning &amp; tool orchestration. 3. LLM Gateway / Model Router LiteLLM Layer proxy untuk unify API dari berbagai LLM (OpenAI, Claude, Gemini, Mistral, Ollama, dll). Dapat melakukan load-balancing, logging, dan cost control. OpenRouter, Helicone, LMQL + Bisa ganti model tanpa ubah kode (API OpenAI-compatible)+ Logging, rate limit, &amp; cost tracking+ Integrasi langsung dengan LangChain &amp; LangServe OSS (gratis) dengan opsi enterprise. Gunakan untuk kontrol biaya &amp; multi-model routing. Contoh: GPT-4 untuk reasoning, Mistral untuk summarization. 4. Observability / Evaluasi / Logging LangSmith Monitoring, tracing, evaluasi, dan debugging interaksi LLM. LangFuse, LangWatch, PromptLayer + UI visual untuk tracing chain/agent+ Evaluator otomatis berbasis LLM-as-judge Freemium. Sangat berguna untuk audit hasil feedback &amp; error tracing agent HR. 5. Deployment &amp; Serving Layer LangServe (FastAPI-based) Menyediakan endpoint API siap pakai untuk chain/agent dengan dokumentasi otomatis &amp; streaming. BentoML, FastAPI Native + Auto OpenAPI docs+ Integrasi LangSmith &amp; LiteLLM+ Streaming realtime OSS. Gunakan jika ingin expose agent ke frontend HR atau internal API. 6. Vector Database (RAG Layer) Qdrant, Chroma, Pinecone, FAISS, Weaviate Menyimpan embedding dokumen HR &amp; KPI untuk pencarian semantik. Milvus, Vespa + Query cepat &amp; integrasi langsung ke LangChain Pinecone (freemium, SaaS), Qdrant (OSS). Pilih sesuai kebutuhan hosting. 7. Relational / Document Database PostgreSQL / MongoDB Menyimpan data user, KPI, hasil evaluasi, dan riwayat feedback. Supabase (Postgres SaaS), Firestore + Stabil &amp; mudah integrasi+ Banyak ORM Python OSS. Gunakan PostgreSQL untuk struktur kuat; MongoDB untuk fleksibilitas data JSON. 8. Cache / Session / Message Queue Redis Menyimpan session context, cache hasil model, dan event asinkron antar agent. Memcached, RabbitMQ, Kafka + Cepat &amp; efisien+ Dukungan pub/sub untuk state LangGraph OSS. Wajib untuk multi-session agent agar context tidak hilang. 9. Embedding Generator OpenAI Embeddings / SentenceTransformers / LiteLLM embedding router Mengubah teks \u2192 vektor numerik untuk pencarian semantik. HuggingFace Embeddings, Cohere + Akurasi tinggi+ Bisa dikelola lewat LiteLLM untuk unify API Cost kecil ($0.0001/1K token). Bisa lokal via SentenceTransformers untuk privasi data HR. 10. Backend Framework (Python) FastAPI API server utama, user management, dan orchestrator AI agent. Flask, Django REST + Asynchronous &amp; performa tinggi+ Native integrasi ke LangServe OSS. Ideal untuk production-grade AI API backend. 11. Inter-Agent Protocol / Standard Context MCP (Model Context Protocol) Protokol standar untuk pertukaran konteks antar agent, tool, IDE, dan app eksternal. OpenAI MCP SDK, GitHub Copilot MCP + Standardisasi komunikasi antar agent &amp; alat eksternal+ Plug-and-play antar sistem AI OSS (open standard). Berguna untuk integrasi dengan sistem HRIS, CRM, atau Copilot internal."},{"location":"tech-stack/overview/","title":"Overview","text":"<p>Berikut ini saya buat daftar tech stack lengkap yang biasanya dibutuhkan untuk membangun sistem AI Agent modern.</p>"},{"location":"tech-stack/overview/#1-llm-model-layer","title":"\ud83e\udde0 1. LLM &amp; Model Layer","text":"<p>Ini \u201cotak\u201d utama dari AI agent.</p> Kategori Fungsi Contoh Tools / Provider Catatan LLM Provider (Commercial) Bahasa, reasoning, generation OpenAI GPT-4/4o, Anthropic Claude 3, Gemini 1.5, Mistral Large Cocok untuk produksi cepat, API stabil. Biaya tergantung token. LLM Open Source (Local/Private) Privasi, kontrol penuh LLaMA 3, Mistral 7B/8x22B, Falcon, Mixtral, Phi-3, DeepSeek Perlu GPU (NVIDIA A100/H100) atau inference server (vLLM, Ollama, Text-Generation-Inference). Middleware / Abstraction Layer Menyediakan \u201crouter\u201d ke banyak model sekaligus OpenRouter, LiteLLM, vLLM, Ollama, LM Studio Cocok kalau kamu ingin fleksibilitas memilih model (misalnya: pakai Claude untuk reasoning, GPT-4o untuk summarization)."},{"location":"tech-stack/overview/#2-knowledge-memory-layer","title":"\ud83d\uddc2\ufe0f 2. Knowledge &amp; Memory Layer","text":"<p>Agent butuh context retention dan retrieval agar bisa \u201cingat\u201d.</p> Kategori Fungsi Tools / Framework Catatan Vector Database Menyimpan embedding dokumen, memori percakapan Weaviate, Pinecone, Qdrant, Milvus, Chroma Untuk RAG (Retriever-Augmented Generation). Pilih yang cocok dengan beban &amp; harga. Graph Database (optional) Hubungan antar entitas (misal user \u2192 transaksi \u2192 lokasi) Neo4j, Memgraph, ArangoDB Berguna untuk reasoning berbasis hubungan antar data. Key-Value Store / Cache Cache hasil embedding / query Redis, Memcached Untuk percepatan response time &amp; menghemat biaya model."},{"location":"tech-stack/overview/#3-agent-framework-orchestration-layer","title":"\ud83e\udde9 3. Agent Framework / Orchestration Layer","text":"<p>Tempat logika utama agent berjalan.</p> Kategori Fungsi Tools Catatan LLM Orchestration / Framework Membangun agent dengan memory, tools, reasoning, state LangChain, LangGraph, Semantic Kernel, CrewAI, AutoGen, Haystack, Rasa Biasanya di sinilah \u201cintelligence logic\u201d berada. Workflow / Task Orchestration Mengatur urutan kerja antar agent atau task Temporal.io, Airflow, Prefect, Celery Berguna untuk proses kompleks (pipeline multi-step, retry, scheduling). Tool Execution Layer Eksekusi perintah nyata (search, call API, DB query) LangChain Tools, ReAct pattern, custom plugins, OpenAI Tools API Agent bisa \u201cmengambil tindakan nyata\u201d melalui layer ini."},{"location":"tech-stack/overview/#4-backend-api-layer","title":"\ud83d\udcbe 4. Backend / API Layer","text":"<p>Tempat semua logika bisnis dan integrasi dihosting.</p> Layer Fungsi Tools / Framework Catatan API Backend REST / GraphQL / gRPC API untuk client FastAPI (Python), Fiber (Go), Express.js / NestJS (Node.js) Cocok sebagai penghubung antara frontend &amp; agent logic. Authentication &amp; Access Control Pengaturan user, token, RBAC, audit Auth0, Keycloak, Supabase Auth, Firebase Auth Jangan lupa audit log &amp; token rotation untuk keamanan. Background Jobs / Queue Asynchronous tasks (e.g., email, batch scoring) RabbitMQ, Kafka, Redis Queue, Celery Supaya agent bisa delegasi pekerjaan berat di background."},{"location":"tech-stack/overview/#5-data-observability-layer","title":"\ud83d\udca1 5. Data &amp; Observability Layer","text":"<p>Untuk monitoring, tracing, debugging, dan evaluasi performa agent.</p> Layer Fungsi Tools / Framework Catatan Tracing &amp; Debugging Melihat langkah reasoning, panggilan API LangSmith, LangWatch, Helicone Penting untuk evaluasi &amp; audit reasoning. Logging &amp; Metrics Menyimpan log agent &amp; metrik performa Prometheus + Grafana, ELK Stack, OpenTelemetry Wajib untuk produksi besar agar tahu bottleneck. Eval Framework Mengukur kualitas jawaban agent LangSmith eval, DeepEval, Ragas, OpenAI Evals Evaluasi otomatis terhadap respons agent."},{"location":"tech-stack/overview/#6-frontend-interface-layer","title":"\ud83c\udf10 6. Frontend / Interface Layer","text":"<p>Cara user berinteraksi dengan agent.</p> Tipe Contoh Tools Web Dashboard / Chat UI Chat interface, analytics, CMS rules React.js + Tailwind + shadcn/ui, Next.js, Svelte Biasanya diintegrasikan via API ke backend. Internal Dashboard / Admin UI Untuk monitoring, pengaturan rule agent React Admin, Superset, Retool, Streamlit Bisa jadi UI CMS internal seperti yang kamu pakai di sistem deteksi fraud. Integrasi Chat / Voice WhatsApp, Telegram, Slack, Twilio Voice Botpress, Twilio API, WhatsApp Cloud API Cocok jika agent diakses lewat channel eksternal."},{"location":"tech-stack/overview/#7-infrastructure-deployment-layer","title":"\ud83e\uddf1 7. Infrastructure &amp; Deployment Layer","text":"<p>Untuk menjalankan agent di production environment.</p> Komponen Fungsi Tools / Platform Catatan Containerization Packaging aplikasi Docker, Podman Memudahkan deployment. Orchestration Skalabilitas &amp; reliability Kubernetes (K8s), Docker Swarm Untuk deployment skala besar. Cloud / Hosting Tempat menjalankan layanan AWS, GCP, Azure, Vultr, Fly.io, Railway.app Pilih tergantung biaya &amp; kebutuhan GPU. GPU / Model Serving Infra Menjalankan model open source vLLM, HuggingFace TGI, Ollama, Replicate, Sagemaker Jika kamu ingin model open-source sendiri. CI/CD Deployment otomatis GitHub Actions, GitLab CI, ArgoCD Pastikan pipeline untuk testing &amp; deployment stabil."},{"location":"tech-stack/overview/#8-security-governance-layer","title":"\ud83d\udd10 8. Security &amp; Governance Layer","text":"<p>Sering diabaikan, tapi penting untuk agent enterprise.</p> Area Tools / Praktik Catatan Data Governance &amp; Masking Presidio, PII filter, custom middlewares Hindari kebocoran data sensitif ke LLM. Model Output Guardrails Guardrails AI, Rebuff, Azure Content Filter Validasi &amp; filter output dari LLM. Audit Trail &amp; Compliance Audit logs + event sourcing Penting untuk regulasi (OJK, GDPR, dll). API Security OAuth2, JWT, mTLS Wajib untuk sistem multi-tenant."},{"location":"tech-stack/overview/#9-ml-analytics-layer-opsional-tapi-kuat","title":"\ud83e\udde0 9. ML / Analytics Layer (Opsional tapi kuat)","text":"<p>Untuk agent yang belajar atau beradaptasi.</p> Kategori Fungsi Tools Data Processing / ETL Menyiapkan data untuk agent Pandas, Polars, Apache Spark, Airbyte Model Training / Fine-tuning Adaptasi model LLM kecil HuggingFace Transformers, PEFT, LoRA, Axolotl ML Pipeline Management Versioning model &amp; data MLflow, Weights &amp; Biases, DVC Feature Store (untuk scoring) Menyimpan fitur prediksi Feast, Tecton"},{"location":"tech-stack/overview/#10-dev-tools-sdk","title":"\u2699\ufe0f 10. Dev Tools / SDK","text":"<p>Supaya development lebih efisien:</p> <ul> <li>LangSmith SDK, OpenAI SDK, Anthropic SDK</li> <li>Postman / Bruno (API testing)</li> <li>VSCode + Copilot / Cody / Continue.dev</li> <li>Docker Compose + Tilt / Airplane / Railway</li> <li>pytest + coverage untuk test agent behavior</li> </ul>"},{"location":"tech-stack/overview/#contoh-stack-kombinasi-nyata-rekomendasi-production","title":"\ud83d\ude80 Contoh Stack Kombinasi Nyata (Rekomendasi Production)","text":"Layer Tools yang disarankan Backend Core FastAPI + LangChain + LangGraph Vector DB Qdrant (open-source, fast, ringan) LLM Router LiteLLM atau OpenRouter Observability LangSmith + OpenTelemetry Infra Docker + Kubernetes + Prometheus Frontend Next.js + Tailwind + shadcn/ui Auth Auth0 / Keycloak Security Presidio + Guardrails Deployment Railway / AWS ECS / GCP CloudRun Data Postgres + Redis cache Optional ML HuggingFace + MLflow (untuk custom fine-tune)"}]}